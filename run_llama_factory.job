#!/bin/bash

#SBATCH -p compute
#SBATCH -J LF_L_3.1_8B
#SBATCH --cpus-per-task=4
#SBATCH --mem=200000
#SBATCH --gres=gpu:a100:1
#SBATCH -t 24:00:00
#SBATCH -o LF_%j.out
#SBATCH -e LF_%j.err

# ============= CONFIGURATION =============
# Set to "true" to skip training and run inference+eval on existing model
# Set to "false" to run full train+infer+eval pipeline
SKIP_TRAIN="false"
# Timestamp of existing model/predictions when SKIP_TRAIN="true"
OLD_TRAIN_TIMESTAMP="2025-01-17-17-56-34"
# 2025-01-17-09-43-52
# baseline

# Set to "true" to skip inference and run eval on existing predictions only
# Set to "false" to run inference+eval
SKIP_INFERENCE="false"

# Training parameters
MAX_SAMPLES=30000
BATCH_SIZE=64 #32
GRAD_ACCUM=32
NUM_EPOCHS=3
MONITOR_PERCENT=10
CLIENT="DOMI" # "BALS" or "DOMI"
TARGET_LANG="de" # if DOMI, "ja" or "de", if BALS, "de" only
# Calculate steps
TOTAL_STEPS=$((MAX_SAMPLES * NUM_EPOCHS / (BATCH_SIZE * GRAD_ACCUM)))
MONITOR_STEPS=$((TOTAL_STEPS * MONITOR_PERCENT / 100))
SAVE_STEPS=$((MONITOR_STEPS * 4)) 
# =======================================

# Initialize conda
eval "$($HOME/miniconda3/bin/conda shell.bash hook)"

# Create and activate environment
# conda create -n lf-env python=3.11 -y
conda activate lf-env10

# Only install the following packages once on the environment
# Install CUDA-enabled PyTorch first
# conda install -y pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia

# Install bitsandbytes with CUDA support
# pip install bitsandbytes==0.43.1 --prefer-binary
# pip install wandb
# pip install liger-kernel>=0.3.0
# pip install sacrebleu unbabel-comet

# Clone and setup LLaMA Factory ONLY on the first run
cd ~ || exit 1
# rm -rf LLaMA-Factory
# git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git
cd LLaMA-Factory

# Only install requirements once on the environment
# pip install -r requirements.txt
# pip install -e .[torch,bitsandbytes,metrics]


# Quick GPU check
python -c "import torch; print('CUDA available?', torch.cuda.is_available()); print('Device count:', torch.cuda.device_count());"

# Set timestamp once at the beginning
export TIMESTAMP=$(date +%Y-%m-%d-%H-%M-%S)
MODEL_PATH="saves/Llama-3.1-8B-Instruct/lora/${CLIENT}_${TARGET_LANG}_train_${TIMESTAMP}"

# Print configuration for logging
echo "----------------------------------------"
echo "Training configuration:"
echo "Client: ${CLIENT}"
echo "Language: ${TARGET_LANG}"
echo "Max samples: ${MAX_SAMPLES}"
echo "Total training steps: ${TOTAL_STEPS}"
echo "Monitor percentage: ${MONITOR_PERCENT}%"
echo "Monitor steps: ${MONITOR_STEPS}"
echo "Save steps: ${SAVE_STEPS}"
echo "----------------------------------------"

# Create training config with variable
cat <<EOF > train_llama3.json
{
  "stage": "sft",
  "do_train": true,
  "model_name_or_path": "meta-llama/Meta-Llama-3.1-8B-Instruct",
  "dataset": "${CLIENT}_${TARGET_LANG}_train_dataset",
  "dataset_dir": "data",
  "template": "llama3",
  "finetuning_type": "lora",
  "lora_target": "all",
  "output_dir": "saves/Llama-3.1-8B-Instruct/lora/${CLIENT}_${TARGET_LANG}_train_${TIMESTAMP}",
  "bf16": true,
  "cutoff_len": 512,
  "packing": false,  
  "ddp_timeout": 180000000,
  "eval_steps": ${MONITOR_STEPS},
  "eval_strategy": "steps",
  "flash_attn": "auto",
  "gradient_accumulation_steps": ${GRAD_ACCUM},
  "learning_rate": 0.001,
  "logging_steps": ${MONITOR_STEPS},
  "lora_alpha": 128,
  "lora_dropout": 0.3,
  "lora_rank": 64,
  "lr_scheduler_type": "polynomial",
  "max_grad_norm": 0.5,
  "max_samples": ${MAX_SAMPLES},
  "num_train_epochs": ${NUM_EPOCHS},
  "optim": "adamw_torch",
  "packing": false,
  "per_device_eval_batch_size": 8,
  "per_device_train_batch_size": ${BATCH_SIZE},
  "plot_loss": true,
  "preprocessing_num_workers": 16,
  "report_to": "wandb",
  "run_name": "${CLIENT}_${TARGET_LANG}_train_${TIMESTAMP}",
  "save_steps": ${SAVE_STEPS},
  "trust_remote_code": true, 
  "upcast_layernorm": true,
  "val_size": 0.1,
  "warmup_ratio": 0.05,
  "weight_decay": 0.01
}
EOF

#  "quantization_bit": 4,
#  "quantization_method": "bitsandbytes",

#   "cutoff_len": 280,
#   "packing": true,

# upcast_layernorm:  Whether or not to upcast the layernorm weights in fp32 (remove when 4bit quantization is used)
#   // "use_unsloth": true,
#   // "flash_attn": fa2,
#   // "enable_liger_kernel": true

# Load all API keys
if [ ! -f ~/.api_keys ]; then
    echo "Error: ~/.api_keys not found!"
    echo "Check run_llama_factory.job for API keys loading - ~/.wandb_token and ~/.huggingface_token"
    exit 1
fi
# Source the API keys
source ~/.api_keys

cd ~/LLaMA-Factory || exit 1

# At the top with other exports
export CLIENT
export TARGET_LANG

if [ "$SKIP_TRAIN" = "false" ]; then
    echo "Running full pipeline: train + inference + evaluation"
    llamafactory-cli train train_llama3.json
else
    echo "Using timestamp from existing run: ${OLD_TRAIN_TIMESTAMP}"
    TIMESTAMP=$OLD_TRAIN_TIMESTAMP
fi

if [ "$SKIP_INFERENCE" = "false" ]; then
    echo "Running inference..."
    python scripts/vllm_infer.py \
        --model_name_or_path "meta-llama/Meta-Llama-3.1-8B-Instruct" \
        --adapter_name_or_path "/home/ivieira/LLaMA-Factory/saves/Llama-3.1-8B-Instruct/lora/${CLIENT}_${TARGET_LANG}_train_${TIMESTAMP}" \
        --dataset ${CLIENT}_${TARGET_LANG}_test_dataset \
        --dataset_dir "data" \
        --template llama3 \
        --temperature 1.0 \
        --top_p 1.0 \
        --vllm_config '{"enable_lora": true, "max_lora_rank": 64, "max_loras": 1}' \
        --save_name "/home/ivieira/LLaMA-Factory/saves/Llama-3.1-8B-Instruct/lora/${CLIENT}_${TARGET_LANG}_train_${TIMESTAMP}/${CLIENT}_${TARGET_LANG}_predictions_${TIMESTAMP}.json"
fi

# Run evaluation with TIMESTAMP available in environment
echo "Running evaluation..."
TIMESTAMP=${TIMESTAMP} CLIENT=${CLIENT} TARGET_LANG=${TARGET_LANG} python ~/chicago2/HP_FT_TM/inference_eval.py \
    --predictions_file "/home/ivieira/LLaMA-Factory/saves/Llama-3.1-8B-Instruct/lora/${CLIENT}_${TARGET_LANG}_train_${TIMESTAMP}/${CLIENT}_${TARGET_LANG}_predictions_${TIMESTAMP}.json"

echo "----------------------------------------"
echo "Pipeline completed with settings:"
echo "SKIP_TRAIN: ${SKIP_TRAIN}"
echo "SKIP_INFERENCE: ${SKIP_INFERENCE}"
echo "TIMESTAMP: ${TIMESTAMP}"
echo "Predictions path: /home/ivieira/LLaMA-Factory/saves/Llama-3.1-8B-Instruct/lora/${CLIENT}_${TARGET_LANG}_train_${TIMESTAMP}/${CLIENT}_${TARGET_LANG}_predictions_${TIMESTAMP}.json"
echo "----------------------------------------"

# Print config files for reproducibility
echo "----------------------------------------"
echo "Config files used:"
echo "Training config:"
cat train_llama3.json
echo "----------------------------------------"