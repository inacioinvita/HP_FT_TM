#!/bin/bash

#SBATCH -p compute
#SBATCH -J sage_train_inf_eval_job
#SBATCH --cpus-per-task=4
#SBATCH --mem=200000
#SBATCH --nodelist=g129
#SBATCH -t 8:00:00
#SBATCH --gres=gpu:a100:4

# Convert line endings to Unix format
# sed -i.bak 's/\r$//' "$0"

# Navigate to the project directory
cd ~/chicago2/HP_FT_TM

# Pull the latest changes from the repository
git fetch origin
git checkout origin/main -- run_sage_train_inf_eval.job sage_train_inf_eval.py
git pull origin main # Replace "main" with the name of your default branch if it's different


# Load necessary modules (modify as per your cluster's configuration)
source /etc/profile.d/modules.sh
module load python/3.8
module load cuda/12.5

# Initialize conda for bash
eval "$(/home/ivieira/mambaforge/bin/conda shell.bash hook)"

# Set up conda/mamba environment paths
export PATH="/home/ivieira/mambaforge/bin:$PATH"
export CONDA_EXE="/home/ivieira/mambaforge/bin/conda"
export CONDA_PYTHON_EXE="/home/ivieira/mambaforge/bin/python"
export CONDA_SHLVL="1"

# Initialize conda
source /home/ivieira/mambaforge/etc/profile.d/conda.sh

# Create the environment if it doesn't exist
if ! conda env list | grep -q "my-pip-env"; then
    conda create -n my-pip-env python=3.8 -y
fi

# Activate the environment
conda activate my-pip-env

# Set up MLflow
export MLFLOW_TRACKING_URI="file:///home/ivieira/chicago2/mlruns"
export MLFLOW_EXPERIMENT_NAME="adobe_training_inference_evaluation"

# Instead of starting the MLflow server, we'll just use the file store
# Remove or comment out the following lines:
# mlflow server \
#     --backend-store-uri sqlite:///mlflow.db \
#     --default-artifact-root ./mlruns \
#     --host 0.0.0.0 \
#     --port 5000 &


# Ensure pip is up-to-date
python -m pip install --upgrade pip -q

# Install required packages without using --user to prevent conflicts within Conda
python -m pip install \
    torch \
    ctranslate2==4.3.0 \
    transformers==4.37.0 \
    huggingface_hub \
    accelerate \
    sentencepiece \
    sacrebleu \
    pandas \
    comet \
    unbabel-comet \
    polars \
    bitsandbytes \
    mlflow \
    trl \
    peft \
    datasets \
    safetensors

# Define paths
PROJECT_DIR=~/chicago2/
SCRIPT_PATH=~/chicago2/HP_FT_TM/sage_train_inf_eval.py

# Set seed
# SEED=42

# Get the current date to use in the checkpoint directory
CURRENT_DATE=$(date +%Y%m%d_%H%M%S)

# Define adjustable parameters

# Hyperparameters
TARGET_LANG="pt-br"
NUM_TRAIN_EPOCHS=1
TRAIN_BATCH_SIZE=32
EVAL_BATCH_SIZE=32
LEARNING_RATE=1e-3
LORA_ALPHA=16
LORA_DROPOUT=0.1
LORA_R=64

# Training Details
LOGGING_STEPS=250
SAVE_STEPS=1000
EVALUATION_STRATEGY="steps"
EVAL_STEPS=1000
SAVE_TOTAL_LIMIT=3

# qLoRA Details
LOAD_IN_4BIT="true"
BNB_4BIT_QUANT_TYPE="nf4"
BNB_4BIT_USE_DOUBLE_QUANT="true"

# CTranslate2 Details
CT2_QUANTIZATION="int8"
CT2_COMPUTE_TYPE="int8"
MAX_LENGTH=512

# Evaluation Steps
EVAL_STEPS=1000

# File paths
TRAIN_SOURCE="/home/ivieira/chicago2/train_dataset.en"
TRAIN_TARGET="/home/ivieira/chicago2/train_dataset.pt-br"
EVAL_SOURCE="/home/ivieira/chicago2/eval_dataset.en"
EVAL_TARGET="/home/ivieira/chicago2/eval_dataset.pt-br"
TEST_SOURCE="/home/ivieira/chicago2/test_dataset.en"
TEST_TARGET="/home/ivieira/chicago2/test_dataset.pt-br"
TRANSLATIONS_FILE="/home/ivieira/chicago2/results/inference/translations_$CURRENT_DATE.txt"

# Other parameters
NUM_TRAIN_RECORDS=-1  # Use all training records
FULL="full"
OUTPUT_DIR="/home/ivieira/chicago2/models/fine_tuned_models/llama-3-8B-$TARGET_LANG-$CURRENT_DATE"
mkdir -p "$OUTPUT_DIR"
# MLFlow parameters
MLFLOW_TRACKING_URI="file:///home/ivieira/chicago2/mlruns"
MLFLOW_EXPERIMENT_NAME="sage_training_inference_evaluation"

# Export MLFlow environment variables
export MLFLOW_TRACKING_URI=$MLFLOW_TRACKING_URI
export MLFLOW_EXPERIMENT_NAME=$MLFLOW_EXPERIMENT_NAME

MODEL_PATH="/home/ivieira/chicago2/models/meta-llama/Meta-Llama-3-8B-Instruct"

# Define arguments for training
TRAIN_ARGS=(
    --train
    --train_source "$TRAIN_SOURCE"
    --train_target "$TRAIN_TARGET"
    --eval_source "$EVAL_SOURCE"
    --eval_target "$EVAL_TARGET"
    --target_lang "$TARGET_LANG"
    --num_train_records "$NUM_TRAIN_RECORDS"
    --full "$FULL"
    --output_dir "$OUTPUT_DIR"
    --num_train_epochs "$NUM_TRAIN_EPOCHS"
    --train_batch_size "$TRAIN_BATCH_SIZE"
    --eval_batch_size "$EVAL_BATCH_SIZE"
    --learning_rate "$LEARNING_RATE"
    --model_path "$MODEL_PATH"
    --lora_alpha "$LORA_ALPHA"
    --lora_dropout "$LORA_DROPOUT"
    --lora_r "$LORA_R"
    --load_in_4bit "$LOAD_IN_4BIT"
    --bnb_4bit_quant_type "$BNB_4BIT_QUANT_TYPE"
    --bnb_4bit_use_double_quant "$BNB_4BIT_USE_DOUBLE_QUANT"
    --ct2_quantization "$CT2_QUANTIZATION"
    --ct2_compute_type "$CT2_COMPUTE_TYPE"
    --max_length "$MAX_LENGTH"
    --eval_steps "$EVAL_STEPS"
    --logging_steps "$LOGGING_STEPS"
    --save_steps "$SAVE_STEPS"
    --evaluation_strategy "$EVALUATION_STRATEGY"
    --eval_steps "$EVAL_STEPS"
    --save_total_limit "$SAVE_TOTAL_LIMIT"
)

# Define arguments for inference
INFER_ARGS=(
    --infer
    --test_source "$TEST_SOURCE"
    --test_target "$TEST_TARGET"
    --model_path "$OUTPUT_DIR"  # Use the output model from training
    --ct2_quantization "$CT2_QUANTIZATION"
    --ct2_compute_type "$CT2_COMPUTE_TYPE"
    --target_lang "$TARGET_LANG"
    --output_dir "$OUTPUT_DIR"
)

# Define arguments for evaluation
EVAL_ARGS=(
    --evaluate
    --source_test_file "$TEST_SOURCE"
    --target_test_file "$TEST_TARGET"
    --translations_file "$TRANSLATIONS_FILE"
    --data_dir "$PROJECT_DIR"
    --target_lang "$TARGET_LANG"
    --output_dir "$OUTPUT_DIR"
)

# Define new parameters
LOGGING_STEPS=250
SAVE_STEPS=1000
EVALUATION_STRATEGY="steps"
EVAL_STEPS=1000
SAVE_TOTAL_LIMIT=3

# Run training, inference, and evaluation sequentially
python $SCRIPT_PATH "${TRAIN_ARGS[@]}" && \
python $SCRIPT_PATH "${INFER_ARGS[@]}" && \
python $SCRIPT_PATH "${EVAL_ARGS[@]}" > train_and_evaluate_stdout.log 2> train_and_evaluate_stderr.log     




