CUDA available? True
Device count: 1
[WARNING|2025-01-13 12:53:48] llamafactory.hparams.parser:162 >> We recommend enable `upcast_layernorm` in quantized training.
[INFO|2025-01-13 12:53:48] llamafactory.hparams.parser:373 >> Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: torch.bfloat16
[INFO|2025-01-13 12:53:51] llamafactory.data.template:157 >> Add pad token: <|eot_id|>
[INFO|2025-01-13 12:53:51] llamafactory.data.template:157 >> Add <|eot_id|>,<|eom_id|> to stop words.
[INFO|2025-01-13 12:53:51] llamafactory.data.loader:157 >> Loading dataset BALS_de_dataset.json...
----------------------------------------
Model and checkpoint locations:
Base directory: /home/ivieira/LLaMA-Factory/
Model save path: /home/ivieira/LLaMA-Factory/saves/Llama-3.1-8B-Instruct/lora/train_2025-01-13-12-53-52
Checkpoint directory: /home/ivieira/LLaMA-Factory/saves/Llama-3.1-8B-Instruct/lora/train_2025-01-13-12-53-52/checkpoint-*
----------------------------------------
