#!/bin/bash

#SBATCH -p compute
#SBATCH -J t_llam
#SBATCH --cpus-per-task=4
#SBATCH --mem=200000
#SBATCH --nodelist=g128
#SBATCH -t 8:00:00
#SBATCH --gres=gpu:a100:4

# Load CUDA module first
module load cuda/12.5

# 1) Initialize Conda in this shell environment
eval "$(/home/ivieira/mambaforge/bin/conda shell.bash hook)"

# 2) Activate the Conda environment (only once)
conda activate my-pip-env

# 3) Move to your project directory
cd ~/chicago2/HP_FT_TM

# 4) Set CUDA visible devices from SLURM
export CUDA_VISIBLE_DEVICES=$SLURM_STEP_GPUS

# 5) Verify CUDA is available
python -c "import torch; print('CUDA available:', torch.cuda.is_available()); print('Device count:', torch.cuda.device_count()); print('Devices:', [torch.cuda.get_device_name(i) for i in range(torch.cuda.device_count())])"

# 5) Ensure pip is up-to-date
python -m pip install --upgrade pip -q

# 6) Install required packages
python -m pip install --user --upgrade \
    torch \
    ctranslate2==4.3.1 \
    transformers \
    trl \
    huggingface_hub \
    accelerate \
    sentencepiece \
    sacrebleu \
    pandas \
    comet \
    unbabel-comet \
    polars \
    bitsandbytes \
    mlflow \
    peft \
    datasets \
    safetensors

# Verify installations
python -c "import transformers; import trl; print(f'transformers version: {transformers.__version__}'); print(f'trl version: {trl.__version__}')"

# 7) Run the training script
python ~/chicago2/HP_FT_TM/BALS_trainer.py
