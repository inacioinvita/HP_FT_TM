

==> WARNING: A newer version of conda exists. <==
  current version: 23.5.2
  latest version: 24.11.3

Please update conda by running

    $ conda update -n base -c defaults conda

Or to minimize the number of packages updated during conda update use

     conda install conda=24.11.3




==> WARNING: A newer version of conda exists. <==
  current version: 23.5.2
  latest version: 24.11.3

Please update conda by running

    $ conda update -n base -c defaults conda

Or to minimize the number of packages updated during conda update use

     conda install conda=24.11.3


Cloning into 'LLaMA-Factory'...
[INFO|configuration_utils.py:679] 2025-01-10 20:51:14,008 >> loading configuration file config.json from cache at /home/ivieira/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/fd5a4dc328319c1cfe9489eccfb9c6406bdfd469/config.json
[INFO|configuration_utils.py:746] 2025-01-10 20:51:14,011 >> Model config LlamaConfig {
  "_name_or_path": "unsloth/llama-3-8b-Instruct-bnb-4bit",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128009,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 8192,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pad_token_id": 128255,
  "pretraining_tp": 1,
  "quantization_config": {
    "_load_in_4bit": true,
    "_load_in_8bit": false,
    "bnb_4bit_compute_dtype": "bfloat16",
    "bnb_4bit_quant_storage": "uint8",
    "bnb_4bit_quant_type": "nf4",
    "bnb_4bit_use_double_quant": true,
    "llm_int8_enable_fp32_cpu_offload": false,
    "llm_int8_has_fp16_weight": false,
    "llm_int8_skip_modules": null,
    "llm_int8_threshold": 6.0,
    "load_in_4bit": true,
    "load_in_8bit": false,
    "quant_method": "bitsandbytes"
  },
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.1",
  "unsloth_version": "2024.9",
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|tokenization_utils_base.py:2211] 2025-01-10 20:51:14,123 >> loading file tokenizer.json from cache at /home/ivieira/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/fd5a4dc328319c1cfe9489eccfb9c6406bdfd469/tokenizer.json
[INFO|tokenization_utils_base.py:2211] 2025-01-10 20:51:14,123 >> loading file tokenizer.model from cache at None
[INFO|tokenization_utils_base.py:2211] 2025-01-10 20:51:14,123 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2211] 2025-01-10 20:51:14,123 >> loading file special_tokens_map.json from cache at /home/ivieira/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/fd5a4dc328319c1cfe9489eccfb9c6406bdfd469/special_tokens_map.json
[INFO|tokenization_utils_base.py:2211] 2025-01-10 20:51:14,123 >> loading file tokenizer_config.json from cache at /home/ivieira/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/fd5a4dc328319c1cfe9489eccfb9c6406bdfd469/tokenizer_config.json
[INFO|tokenization_utils_base.py:2475] 2025-01-10 20:51:14,520 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:679] 2025-01-10 20:51:15,014 >> loading configuration file config.json from cache at /home/ivieira/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/fd5a4dc328319c1cfe9489eccfb9c6406bdfd469/config.json
[INFO|configuration_utils.py:746] 2025-01-10 20:51:15,016 >> Model config LlamaConfig {
  "_name_or_path": "unsloth/llama-3-8b-Instruct-bnb-4bit",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128009,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 8192,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pad_token_id": 128255,
  "pretraining_tp": 1,
  "quantization_config": {
    "_load_in_4bit": true,
    "_load_in_8bit": false,
    "bnb_4bit_compute_dtype": "bfloat16",
    "bnb_4bit_quant_storage": "uint8",
    "bnb_4bit_quant_type": "nf4",
    "bnb_4bit_use_double_quant": true,
    "llm_int8_enable_fp32_cpu_offload": false,
    "llm_int8_has_fp16_weight": false,
    "llm_int8_skip_modules": null,
    "llm_int8_threshold": 6.0,
    "load_in_4bit": true,
    "load_in_8bit": false,
    "quant_method": "bitsandbytes"
  },
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.1",
  "unsloth_version": "2024.9",
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|tokenization_utils_base.py:2211] 2025-01-10 20:51:15,116 >> loading file tokenizer.json from cache at /home/ivieira/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/fd5a4dc328319c1cfe9489eccfb9c6406bdfd469/tokenizer.json
[INFO|tokenization_utils_base.py:2211] 2025-01-10 20:51:15,116 >> loading file tokenizer.model from cache at None
[INFO|tokenization_utils_base.py:2211] 2025-01-10 20:51:15,116 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2211] 2025-01-10 20:51:15,117 >> loading file special_tokens_map.json from cache at /home/ivieira/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/fd5a4dc328319c1cfe9489eccfb9c6406bdfd469/special_tokens_map.json
[INFO|tokenization_utils_base.py:2211] 2025-01-10 20:51:15,117 >> loading file tokenizer_config.json from cache at /home/ivieira/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/fd5a4dc328319c1cfe9489eccfb9c6406bdfd469/tokenizer_config.json
[INFO|tokenization_utils_base.py:2475] 2025-01-10 20:51:15,504 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 91 examples [00:00, 5189.84 examples/s]
Converting format of dataset:   0%|          | 0/50 [00:00<?, ? examples/s]Converting format of dataset: 100%|██████████| 50/50 [00:00<00:00, 4390.47 examples/s]
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 1000 examples [00:00, 32772.10 examples/s]
Converting format of dataset:   0%|          | 0/50 [00:00<?, ? examples/s]Converting format of dataset: 100%|██████████| 50/50 [00:00<00:00, 3036.58 examples/s]
Running tokenizer on dataset:   0%|          | 0/100 [00:00<?, ? examples/s]Running tokenizer on dataset: 100%|██████████| 100/100 [00:00<00:00, 2296.27 examples/s]
[INFO|configuration_utils.py:679] 2025-01-10 20:51:16,590 >> loading configuration file config.json from cache at /home/ivieira/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/fd5a4dc328319c1cfe9489eccfb9c6406bdfd469/config.json
[INFO|configuration_utils.py:746] 2025-01-10 20:51:16,593 >> Model config LlamaConfig {
  "_name_or_path": "unsloth/llama-3-8b-Instruct-bnb-4bit",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128009,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 8192,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pad_token_id": 128255,
  "pretraining_tp": 1,
  "quantization_config": {
    "_load_in_4bit": true,
    "_load_in_8bit": false,
    "bnb_4bit_compute_dtype": "bfloat16",
    "bnb_4bit_quant_storage": "uint8",
    "bnb_4bit_quant_type": "nf4",
    "bnb_4bit_use_double_quant": true,
    "llm_int8_enable_fp32_cpu_offload": false,
    "llm_int8_has_fp16_weight": false,
    "llm_int8_skip_modules": null,
    "llm_int8_threshold": 6.0,
    "load_in_4bit": true,
    "load_in_8bit": false,
    "quant_method": "bitsandbytes"
  },
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.1",
  "unsloth_version": "2024.9",
  "use_cache": true,
  "vocab_size": 128256
}

[WARNING|quantization_config.py:415] 2025-01-10 20:51:16,700 >> Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.
[INFO|modeling_utils.py:3937] 2025-01-10 20:51:16,831 >> loading weights file model.safetensors from cache at /home/ivieira/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/fd5a4dc328319c1cfe9489eccfb9c6406bdfd469/model.safetensors
[INFO|modeling_utils.py:1670] 2025-01-10 20:51:18,299 >> Instantiating LlamaForCausalLM model under default dtype torch.float16.
[INFO|configuration_utils.py:1096] 2025-01-10 20:51:18,304 >> Generate config GenerationConfig {
  "bos_token_id": 128000,
  "eos_token_id": 128009,
  "pad_token_id": 128255
}

[INFO|modeling_utils.py:4800] 2025-01-10 20:51:23,005 >> All model checkpoint weights were used when initializing LlamaForCausalLM.

[INFO|modeling_utils.py:4808] 2025-01-10 20:51:23,006 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at unsloth/llama-3-8b-Instruct-bnb-4bit.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
[INFO|configuration_utils.py:1051] 2025-01-10 20:51:23,269 >> loading configuration file generation_config.json from cache at /home/ivieira/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/fd5a4dc328319c1cfe9489eccfb9c6406bdfd469/generation_config.json
[INFO|configuration_utils.py:1096] 2025-01-10 20:51:23,269 >> Generate config GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": [
    128001,
    128009
  ],
  "max_length": 8192,
  "pad_token_id": 128255,
  "temperature": 0.6,
  "top_p": 0.9
}

[INFO|trainer.py:698] 2025-01-10 20:51:23,938 >> Using auto half precision backend
[INFO|trainer.py:2313] 2025-01-10 20:51:24,141 >> ***** Running training *****
[INFO|trainer.py:2314] 2025-01-10 20:51:24,141 >>   Num examples = 100
[INFO|trainer.py:2315] 2025-01-10 20:51:24,141 >>   Num Epochs = 1
[INFO|trainer.py:2316] 2025-01-10 20:51:24,141 >>   Instantaneous batch size per device = 2
[INFO|trainer.py:2319] 2025-01-10 20:51:24,141 >>   Total train batch size (w. parallel, distributed & accumulation) = 8
[INFO|trainer.py:2320] 2025-01-10 20:51:24,141 >>   Gradient Accumulation steps = 4
[INFO|trainer.py:2321] 2025-01-10 20:51:24,141 >>   Total optimization steps = 12
[INFO|trainer.py:2322] 2025-01-10 20:51:24,145 >>   Number of trainable parameters = 20,971,520
  0%|          | 0/12 [00:00<?, ?it/s]  8%|▊         | 1/12 [00:02<00:28,  2.57s/it] 17%|█▋        | 2/12 [00:04<00:21,  2.12s/it] 25%|██▌       | 3/12 [00:06<00:17,  1.90s/it] 33%|███▎      | 4/12 [00:07<00:14,  1.78s/it] 42%|████▏     | 5/12 [00:09<00:12,  1.76s/it] 50%|█████     | 6/12 [00:10<00:09,  1.61s/it] 58%|█████▊    | 7/12 [00:12<00:07,  1.57s/it] 67%|██████▋   | 8/12 [00:13<00:06,  1.57s/it] 75%|███████▌  | 9/12 [00:15<00:04,  1.57s/it] 83%|████████▎ | 10/12 [00:16<00:03,  1.58s/it]                                                83%|████████▎ | 10/12 [00:16<00:03,  1.58s/it] 92%|█████████▏| 11/12 [00:18<00:01,  1.55s/it]100%|██████████| 12/12 [00:20<00:00,  1.63s/it][INFO|trainer.py:3801] 2025-01-10 20:51:44,331 >> Saving model checkpoint to llama3_lora/checkpoint-12
[INFO|configuration_utils.py:679] 2025-01-10 20:51:44,540 >> loading configuration file config.json from cache at /home/ivieira/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/fd5a4dc328319c1cfe9489eccfb9c6406bdfd469/config.json
[INFO|configuration_utils.py:746] 2025-01-10 20:51:44,541 >> Model config LlamaConfig {
  "_name_or_path": "unsloth/llama-3-8b-Instruct",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128009,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 8192,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pad_token_id": 128255,
  "pretraining_tp": 1,
  "quantization_config": {
    "_load_in_4bit": true,
    "_load_in_8bit": false,
    "bnb_4bit_compute_dtype": "bfloat16",
    "bnb_4bit_quant_storage": "uint8",
    "bnb_4bit_quant_type": "nf4",
    "bnb_4bit_use_double_quant": true,
    "llm_int8_enable_fp32_cpu_offload": false,
    "llm_int8_has_fp16_weight": false,
    "llm_int8_skip_modules": null,
    "llm_int8_threshold": 6.0,
    "load_in_4bit": true,
    "load_in_8bit": false,
    "quant_method": "bitsandbytes"
  },
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.1",
  "unsloth_version": "2024.9",
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|tokenization_utils_base.py:2646] 2025-01-10 20:51:44,653 >> tokenizer config file saved in llama3_lora/checkpoint-12/tokenizer_config.json
[INFO|tokenization_utils_base.py:2655] 2025-01-10 20:51:44,653 >> Special tokens file saved in llama3_lora/checkpoint-12/special_tokens_map.json
[INFO|trainer.py:2584] 2025-01-10 20:51:44,970 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 12/12 [00:20<00:00,  1.63s/it]100%|██████████| 12/12 [00:20<00:00,  1.74s/it]
[INFO|trainer.py:3801] 2025-01-10 20:51:44,972 >> Saving model checkpoint to llama3_lora
[INFO|configuration_utils.py:679] 2025-01-10 20:51:45,181 >> loading configuration file config.json from cache at /home/ivieira/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/fd5a4dc328319c1cfe9489eccfb9c6406bdfd469/config.json
[INFO|configuration_utils.py:746] 2025-01-10 20:51:45,181 >> Model config LlamaConfig {
  "_name_or_path": "unsloth/llama-3-8b-Instruct",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128009,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 8192,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pad_token_id": 128255,
  "pretraining_tp": 1,
  "quantization_config": {
    "_load_in_4bit": true,
    "_load_in_8bit": false,
    "bnb_4bit_compute_dtype": "bfloat16",
    "bnb_4bit_quant_storage": "uint8",
    "bnb_4bit_quant_type": "nf4",
    "bnb_4bit_use_double_quant": true,
    "llm_int8_enable_fp32_cpu_offload": false,
    "llm_int8_has_fp16_weight": false,
    "llm_int8_skip_modules": null,
    "llm_int8_threshold": 6.0,
    "load_in_4bit": true,
    "load_in_8bit": false,
    "quant_method": "bitsandbytes"
  },
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.1",
  "unsloth_version": "2024.9",
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|tokenization_utils_base.py:2646] 2025-01-10 20:51:45,286 >> tokenizer config file saved in llama3_lora/tokenizer_config.json
[INFO|tokenization_utils_base.py:2655] 2025-01-10 20:51:45,287 >> Special tokens file saved in llama3_lora/special_tokens_map.json
[INFO|modelcard.py:449] 2025-01-10 20:51:45,441 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}
