#!/bin/bash

#SBATCH -p compute
#SBATCH -J API_EVAL
#SBATCH --cpus-per-task=4
#SBATCH --mem=32000
#SBATCH -t 12:00:00
#SBATCH -o API_%j.out
#SBATCH -e API_%j.err

# Initialize conda
eval "$($HOME/miniconda3/bin/conda shell.bash hook)"
conda activate lf-env

# Install required packages if not present
pip install --quiet deepl

cd ~/LLaMA-Factory || exit 1

# Set paths
export TIMESTAMP=$(date +%Y-%m-%d-%H-%M-%S)
DEEPL_DIR="saves/Llama-3.1-8B-Instruct/lora/DeepL_baseline_${TIMESTAMP}"

# Create output directory
mkdir -p $DEEPL_DIR

# Define prediction file
DEEPL_PREDICTIONS="${DEEPL_DIR}/predictions_deepl_${TIMESTAMP}.json"

# Load API keys
source ~/.api_keys

# Run inference for DeepL only
echo "Running inference on DeepL..."
python ~/chicago2/HP_FT_TM/api_infer.py \
    --test_data data/BALS_de_test_dataset.json \
    --google_predictions skip.json \
    --deepl_predictions $DEEPL_PREDICTIONS

# Only run evaluation if prediction file exists
if [ -f "$DEEPL_PREDICTIONS" ]; then
    echo "Running evaluation on DeepL results..."
    python ~/chicago2/HP_FT_TM/inference_eval.py \
        --predictions_file $DEEPL_PREDICTIONS
else
    echo "Warning: DeepL predictions file not found"
fi

echo "----------------------------------------"
echo "API evaluation completed:"
echo "DeepL results: ${DEEPL_DIR}"
echo "----------------------------------------" 