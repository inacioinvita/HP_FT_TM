#!/bin/bash -l

#SBATCH -p compute
#SBATCH --gres=gpu:1
#SBATCH -J API_EVAL
#SBATCH --cpus-per-task=8
#SBATCH --mem=32000
#SBATCH -t 12:00:00
#SBATCH -o API_%j.out
#SBATCH -e API_%j.err

# Create environment variables for CLIENT and TARGET_LANG
export CLIENT="DOMI"
export TARGET_LANG="DE"
export LANG_LOWER=$(echo "$TARGET_LANG" | tr '[:upper:]' '[:lower:]')

# Add environment variable for batch size
export TRANSLATION_BATCH_SIZE=50

# Initialise conda and activate environment
echo "Initialising conda environment..."
eval "$($HOME/miniconda3/bin/conda shell.bash hook)"
conda activate lf-env

source ~/.api_keys

# Debug: Check which Python and torch version are active
echo "Using Python from: $(which python)"
python --version
python -c "import torch; print('Torch version:', torch.__version__)"

# Install required packages quietly
pip install --quiet deepl sacrebleu unbabel-comet

# Change to working directory
cd ~/LLaMA-Factory || { echo "Directory not found!"; exit 1; }

# Set timestamped output directory
export TIMESTAMP=$(date +%Y-%m-%d-%H-%M-%S)
DEEPL_DIR="saves/Llama-3.1-8B-Instruct/lora/${CLIENT}_${TARGET_LANG}_DeepL_baseline_${TIMESTAMP}"
mkdir -p "$DEEPL_DIR"

# Run inference & evaluation in one step
echo "Running DeepL translation and evaluation into ${DEEPL_DIR}..."
python ~/chicago2/HP_FT_TM/api_infer.py \
    --input_file data/${CLIENT}_${LANG_LOWER}_test_dataset.json \
    --output_dir "$DEEPL_DIR"

echo "----------------------------------------"
echo "API inference and evaluation completed."
echo "Results saved to: $DEEPL_DIR"
echo "----------------------------------------"