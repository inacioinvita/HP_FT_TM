[INFO|configuration_utils.py:679] 2025-01-15 15:40:01,558 >> loading configuration file config.json from cache at /home/ivieira/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659/config.json
[INFO|configuration_utils.py:746] 2025-01-15 15:40:01,559 >> Model config LlamaConfig {
  "_name_or_path": "meta-llama/Meta-Llama-3.1-8B-Instruct",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.1",
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|tokenization_utils_base.py:2211] 2025-01-15 15:40:01,770 >> loading file tokenizer.json from cache at /home/ivieira/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659/tokenizer.json
[INFO|tokenization_utils_base.py:2211] 2025-01-15 15:40:01,770 >> loading file tokenizer.model from cache at None
[INFO|tokenization_utils_base.py:2211] 2025-01-15 15:40:01,770 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2211] 2025-01-15 15:40:01,770 >> loading file special_tokens_map.json from cache at /home/ivieira/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659/special_tokens_map.json
[INFO|tokenization_utils_base.py:2211] 2025-01-15 15:40:01,770 >> loading file tokenizer_config.json from cache at /home/ivieira/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659/tokenizer_config.json
[INFO|tokenization_utils_base.py:2475] 2025-01-15 15:40:02,143 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:679] 2025-01-15 15:40:02,939 >> loading configuration file config.json from cache at /home/ivieira/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659/config.json
[INFO|configuration_utils.py:746] 2025-01-15 15:40:02,942 >> Model config LlamaConfig {
  "_name_or_path": "meta-llama/Meta-Llama-3.1-8B-Instruct",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.1",
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|tokenization_utils_base.py:2211] 2025-01-15 15:40:03,139 >> loading file tokenizer.json from cache at /home/ivieira/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659/tokenizer.json
[INFO|tokenization_utils_base.py:2211] 2025-01-15 15:40:03,139 >> loading file tokenizer.model from cache at None
[INFO|tokenization_utils_base.py:2211] 2025-01-15 15:40:03,139 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2211] 2025-01-15 15:40:03,139 >> loading file special_tokens_map.json from cache at /home/ivieira/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659/special_tokens_map.json
[INFO|tokenization_utils_base.py:2211] 2025-01-15 15:40:03,139 >> loading file tokenizer_config.json from cache at /home/ivieira/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659/tokenizer_config.json
[INFO|tokenization_utils_base.py:2475] 2025-01-15 15:40:03,503 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:679] 2025-01-15 15:40:04,264 >> loading configuration file config.json from cache at /home/ivieira/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659/config.json
[INFO|configuration_utils.py:746] 2025-01-15 15:40:04,265 >> Model config LlamaConfig {
  "_name_or_path": "meta-llama/Meta-Llama-3.1-8B-Instruct",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.1",
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|modeling_utils.py:3937] 2025-01-15 15:40:04,343 >> loading weights file model.safetensors from cache at /home/ivieira/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659/model.safetensors.index.json
[INFO|modeling_utils.py:1670] 2025-01-15 15:40:04,343 >> Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.
[INFO|configuration_utils.py:1096] 2025-01-15 15:40:04,345 >> Generate config GenerationConfig {
  "bos_token_id": 128000,
  "eos_token_id": [
    128001,
    128008,
    128009
  ]
}

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|‚ñà‚ñà‚ñå       | 1/4 [00:02<00:08,  2.84s/it]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 2/4 [00:05<00:05,  2.71s/it]Loading checkpoint shards:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 3/4 [00:07<00:02,  2.63s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:08<00:00,  1.82s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:08<00:00,  2.14s/it]
[INFO|modeling_utils.py:4800] 2025-01-15 15:40:13,040 >> All model checkpoint weights were used when initializing LlamaForCausalLM.

[INFO|modeling_utils.py:4808] 2025-01-15 15:40:13,040 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at meta-llama/Meta-Llama-3.1-8B-Instruct.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
[INFO|configuration_utils.py:1051] 2025-01-15 15:40:13,225 >> loading configuration file generation_config.json from cache at /home/ivieira/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659/generation_config.json
[INFO|configuration_utils.py:1096] 2025-01-15 15:40:13,226 >> Generate config GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "temperature": 0.6,
  "top_p": 0.9
}

[INFO|trainer.py:698] 2025-01-15 15:40:15,006 >> Using auto half precision backend
[INFO|trainer.py:2313] 2025-01-15 15:40:15,214 >> ***** Running training *****
[INFO|trainer.py:2314] 2025-01-15 15:40:15,214 >>   Num examples = 23,270
[INFO|trainer.py:2315] 2025-01-15 15:40:15,214 >>   Num Epochs = 1
[INFO|trainer.py:2316] 2025-01-15 15:40:15,214 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:2319] 2025-01-15 15:40:15,214 >>   Total train batch size (w. parallel, distributed & accumulation) = 256
[INFO|trainer.py:2320] 2025-01-15 15:40:15,214 >>   Gradient Accumulation steps = 8
[INFO|trainer.py:2321] 2025-01-15 15:40:15,214 >>   Total optimization steps = 1
[INFO|trainer.py:2322] 2025-01-15 15:40:15,218 >>   Number of trainable parameters = 167,772,160
[INFO|integration_utils.py:812] 2025-01-15 15:40:15,221 >> Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
wandb: Currently logged in as: inaciovieira (inaciovieira-alpha-crc). Use `wandb login --relogin` to force relogin
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.2
wandb: Run data is saved locally in /home/ivieira/LLaMA-Factory/wandb/run-20250115_154015-zg1ll0hw
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run train_2025-01-15-15-39-56
wandb: ‚≠êÔ∏è View project at https://wandb.ai/inaciovieira-alpha-crc/llamafactory
wandb: üöÄ View run at https://wandb.ai/inaciovieira-alpha-crc/llamafactory/runs/zg1ll0hw
  0%|          | 0/1 [00:00<?, ?it/s]/home/ivieira/miniconda3/envs/lf-env/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/home/ivieira/miniconda3/envs/lf-env/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:54<00:00, 54.38s/it][INFO|trainer.py:3801] 2025-01-15 15:41:10,719 >> Saving model checkpoint to saves/Llama-3.1-8B-Instruct/lora/train_2025-01-15-15-39-56/checkpoint-1
[INFO|configuration_utils.py:679] 2025-01-15 15:41:11,158 >> loading configuration file config.json from cache at /home/ivieira/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659/config.json
[INFO|configuration_utils.py:746] 2025-01-15 15:41:11,160 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.1",
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|tokenization_utils_base.py:2646] 2025-01-15 15:41:11,786 >> tokenizer config file saved in saves/Llama-3.1-8B-Instruct/lora/train_2025-01-15-15-39-56/checkpoint-1/tokenizer_config.json
[INFO|tokenization_utils_base.py:2655] 2025-01-15 15:41:11,787 >> Special tokens file saved in saves/Llama-3.1-8B-Instruct/lora/train_2025-01-15-15-39-56/checkpoint-1/special_tokens_map.json
[INFO|trainer.py:2584] 2025-01-15 15:41:12,942 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                             100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:56<00:00, 54.38s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:56<00:00, 56.63s/it]
[INFO|trainer.py:3801] 2025-01-15 15:41:12,959 >> Saving model checkpoint to saves/Llama-3.1-8B-Instruct/lora/train_2025-01-15-15-39-56
[INFO|configuration_utils.py:679] 2025-01-15 15:41:13,510 >> loading configuration file config.json from cache at /home/ivieira/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659/config.json
[INFO|configuration_utils.py:746] 2025-01-15 15:41:13,511 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.1",
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|tokenization_utils_base.py:2646] 2025-01-15 15:41:14,110 >> tokenizer config file saved in saves/Llama-3.1-8B-Instruct/lora/train_2025-01-15-15-39-56/tokenizer_config.json
[INFO|tokenization_utils_base.py:2655] 2025-01-15 15:41:14,111 >> Special tokens file saved in saves/Llama-3.1-8B-Instruct/lora/train_2025-01-15-15-39-56/special_tokens_map.json
[INFO|trainer.py:4117] 2025-01-15 15:41:14,277 >> 
***** Running Evaluation *****
[INFO|trainer.py:4119] 2025-01-15 15:41:14,277 >>   Num examples = 2586
[INFO|trainer.py:4122] 2025-01-15 15:41:14,277 >>   Batch size = 16
/home/ivieira/miniconda3/envs/lf-env/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/home/ivieira/miniconda3/envs/lf-env/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
  0%|          | 0/162 [00:00<?, ?it/s]  1%|          | 2/162 [00:00<00:38,  4.21it/s]  2%|‚ñè         | 3/162 [00:00<00:51,  3.11it/s]  2%|‚ñè         | 4/162 [00:01<01:00,  2.63it/s]  3%|‚ñé         | 5/162 [00:02<01:12,  2.17it/s]  4%|‚ñé         | 6/162 [00:02<01:10,  2.20it/s]  4%|‚ñç         | 7/162 [00:02<01:10,  2.20it/s]  5%|‚ñç         | 8/162 [00:03<01:17,  1.99it/s]  6%|‚ñå         | 9/162 [00:03<01:14,  2.05it/s]  6%|‚ñå         | 10/162 [00:04<01:21,  1.86it/s]  7%|‚ñã         | 11/162 [00:05<01:35,  1.58it/s]  7%|‚ñã         | 12/162 [00:05<01:25,  1.76it/s]  8%|‚ñä         | 13/162 [00:06<01:25,  1.75it/s]  9%|‚ñä         | 14/162 [00:06<01:21,  1.82it/s]  9%|‚ñâ         | 15/162 [00:07<01:16,  1.92it/s] 10%|‚ñâ         | 16/162 [00:07<01:12,  2.01it/s] 10%|‚ñà         | 17/162 [00:08<01:13,  1.98it/s] 11%|‚ñà         | 18/162 [00:09<01:18,  1.84it/s] 12%|‚ñà‚ñè        | 19/162 [00:09<01:15,  1.90it/s] 12%|‚ñà‚ñè        | 20/162 [00:10<01:20,  1.76it/s] 13%|‚ñà‚ñé        | 21/162 [00:10<01:16,  1.85it/s] 14%|‚ñà‚ñé        | 22/162 [00:11<01:10,  1.98it/s] 14%|‚ñà‚ñç        | 23/162 [00:11<01:20,  1.72it/s] 15%|‚ñà‚ñç        | 24/162 [00:12<01:17,  1.79it/s] 15%|‚ñà‚ñå        | 25/162 [00:12<01:13,  1.85it/s] 16%|‚ñà‚ñå        | 26/162 [00:13<01:11,  1.91it/s] 17%|‚ñà‚ñã        | 27/162 [00:14<01:49,  1.24it/s] 17%|‚ñà‚ñã        | 28/162 [00:15<01:35,  1.41it/s] 18%|‚ñà‚ñä        | 29/162 [00:15<01:24,  1.58it/s] 19%|‚ñà‚ñä        | 30/162 [00:16<01:21,  1.63it/s] 19%|‚ñà‚ñâ        | 31/162 [00:16<01:15,  1.73it/s] 20%|‚ñà‚ñâ        | 32/162 [00:17<01:10,  1.83it/s] 20%|‚ñà‚ñà        | 33/162 [00:17<01:07,  1.90it/s] 21%|‚ñà‚ñà        | 34/162 [00:18<01:15,  1.70it/s] 22%|‚ñà‚ñà‚ñè       | 35/162 [00:18<01:10,  1.81it/s] 22%|‚ñà‚ñà‚ñè       | 36/162 [00:20<01:48,  1.16it/s] 23%|‚ñà‚ñà‚ñé       | 37/162 [00:21<01:33,  1.33it/s] 23%|‚ñà‚ñà‚ñé       | 38/162 [00:21<01:24,  1.47it/s] 24%|‚ñà‚ñà‚ñç       | 39/162 [00:22<01:24,  1.45it/s] 25%|‚ñà‚ñà‚ñç       | 40/162 [00:22<01:14,  1.64it/s] 25%|‚ñà‚ñà‚ñå       | 41/162 [00:23<01:14,  1.63it/s] 26%|‚ñà‚ñà‚ñå       | 42/162 [00:23<01:07,  1.79it/s] 27%|‚ñà‚ñà‚ñã       | 43/162 [00:24<01:03,  1.87it/s] 27%|‚ñà‚ñà‚ñã       | 44/162 [00:24<01:09,  1.69it/s] 28%|‚ñà‚ñà‚ñä       | 45/162 [00:25<01:05,  1.79it/s] 28%|‚ñà‚ñà‚ñä       | 46/162 [00:25<01:01,  1.88it/s] 29%|‚ñà‚ñà‚ñâ       | 47/162 [00:26<01:01,  1.86it/s] 30%|‚ñà‚ñà‚ñâ       | 48/162 [00:26<01:01,  1.87it/s] 30%|‚ñà‚ñà‚ñà       | 49/162 [00:27<00:57,  1.95it/s] 31%|‚ñà‚ñà‚ñà       | 50/162 [00:27<00:58,  1.93it/s] 31%|‚ñà‚ñà‚ñà‚ñè      | 51/162 [00:28<00:55,  2.00it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 52/162 [00:28<00:54,  2.02it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 53/162 [00:29<00:52,  2.07it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 54/162 [00:29<00:52,  2.08it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 55/162 [00:30<00:49,  2.16it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 56/162 [00:30<00:49,  2.13it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 57/162 [00:31<00:51,  2.04it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 58/162 [00:31<00:50,  2.07it/s] 36%|‚ñà‚ñà‚ñà‚ñã      | 59/162 [00:32<00:52,  1.95it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 60/162 [00:32<00:52,  1.95it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 61/162 [00:33<00:50,  1.99it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 62/162 [00:33<00:49,  2.03it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 63/162 [00:34<00:47,  2.06it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 64/162 [00:34<00:50,  1.95it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 65/162 [00:35<00:49,  1.96it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 66/162 [00:35<00:47,  2.01it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 67/162 [00:36<00:48,  1.96it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 68/162 [00:36<00:45,  2.05it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 69/162 [00:37<00:47,  1.97it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 70/162 [00:37<00:46,  1.97it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 71/162 [00:38<00:44,  2.04it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 72/162 [00:39<00:56,  1.59it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 73/162 [00:39<00:51,  1.71it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 74/162 [00:40<00:49,  1.79it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 75/162 [00:40<00:45,  1.90it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 76/162 [00:41<00:47,  1.80it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 77/162 [00:41<00:50,  1.69it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 78/162 [00:44<01:26,  1.03s/it] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 79/162 [00:44<01:10,  1.18it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 80/162 [00:44<01:02,  1.31it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 81/162 [00:45<00:55,  1.46it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 82/162 [00:45<00:49,  1.60it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 83/162 [00:46<00:44,  1.76it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 84/162 [00:46<00:42,  1.86it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 85/162 [00:47<00:38,  1.98it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 86/162 [00:47<00:39,  1.95it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 87/162 [00:48<00:46,  1.61it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 88/162 [00:49<00:43,  1.71it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 89/162 [00:49<00:39,  1.84it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 90/162 [00:50<00:39,  1.83it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 91/162 [00:50<00:37,  1.88it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 92/162 [00:51<00:35,  1.97it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 93/162 [00:51<00:33,  2.04it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 94/162 [00:52<00:33,  2.05it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 95/162 [00:52<00:31,  2.11it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 96/162 [00:53<00:32,  2.06it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 97/162 [00:53<00:31,  2.04it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 98/162 [00:54<00:31,  2.06it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 99/162 [00:54<00:29,  2.14it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 100/162 [00:54<00:30,  2.05it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 101/162 [00:55<00:29,  2.08it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 102/162 [00:55<00:28,  2.12it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 103/162 [00:56<00:29,  1.98it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 104/162 [00:56<00:28,  2.01it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 105/162 [00:57<00:28,  2.01it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 106/162 [00:57<00:26,  2.08it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 107/162 [00:58<00:25,  2.15it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 108/162 [00:58<00:25,  2.15it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 109/162 [00:59<00:24,  2.15it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 110/162 [00:59<00:24,  2.14it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 111/162 [01:00<00:23,  2.18it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 112/162 [01:00<00:23,  2.11it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 113/162 [01:01<00:22,  2.15it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 114/162 [01:01<00:21,  2.22it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 115/162 [01:02<00:21,  2.17it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 116/162 [01:02<00:20,  2.26it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 117/162 [01:02<00:20,  2.18it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 118/162 [01:03<00:20,  2.12it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 119/162 [01:03<00:21,  2.03it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 120/162 [01:04<00:20,  2.08it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 121/162 [01:04<00:20,  2.00it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 122/162 [01:05<00:20,  2.00it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 123/162 [01:05<00:19,  2.02it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 124/162 [01:06<00:18,  2.03it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 125/162 [01:06<00:18,  2.00it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 126/162 [01:07<00:17,  2.06it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 127/162 [01:07<00:16,  2.11it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 128/162 [01:08<00:16,  2.07it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 129/162 [01:08<00:16,  2.04it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 130/162 [01:09<00:14,  2.14it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 131/162 [01:09<00:15,  1.97it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 132/162 [01:10<00:15,  1.99it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 133/162 [01:10<00:14,  1.99it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 134/162 [01:11<00:13,  2.05it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 135/162 [01:11<00:12,  2.10it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 136/162 [01:12<00:12,  2.03it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 137/162 [01:12<00:12,  1.94it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 138/162 [01:13<00:11,  2.01it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 139/162 [01:13<00:11,  2.08it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 140/162 [01:14<00:10,  2.15it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 141/162 [01:14<00:10,  2.10it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 142/162 [01:15<00:09,  2.05it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 143/162 [01:15<00:09,  2.03it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 144/162 [01:16<00:09,  1.96it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 145/162 [01:16<00:08,  2.01it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 146/162 [01:17<00:07,  2.07it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 147/162 [01:17<00:08,  1.77it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 148/162 [01:18<00:07,  1.86it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 149/162 [01:18<00:06,  1.96it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 150/162 [01:19<00:06,  1.98it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 151/162 [01:19<00:05,  2.02it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 152/162 [01:20<00:05,  1.91it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 153/162 [01:20<00:04,  1.97it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 154/162 [01:21<00:03,  2.03it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 155/162 [01:21<00:03,  1.99it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 156/162 [01:22<00:02,  2.08it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 157/162 [01:22<00:02,  1.85it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 158/162 [01:23<00:02,  1.96it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 159/162 [01:23<00:01,  2.00it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 160/162 [01:24<00:00,  2.00it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 161/162 [01:24<00:00,  1.98it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 162/162 [01:25<00:00,  2.20it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 162/162 [01:25<00:00,  1.90it/s]
[INFO|modelcard.py:449] 2025-01-15 15:42:40,066 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}
wandb: Currently logged in as: inaciovieira (inaciovieira-alpha-crc). Use `wandb login --relogin` to force relogin
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.2
wandb: Run data is saved locally in /home/ivieira/LLaMA-Factory/wandb/run-20250115_154246-zg1ll0hw
wandb: Run `wandb offline` to turn off syncing.
wandb: Resuming run train_2025-01-15-15-39-56
wandb: ‚≠êÔ∏è View project at https://wandb.ai/inaciovieira-alpha-crc/llamafactory
wandb: üöÄ View run at https://wandb.ai/inaciovieira-alpha-crc/llamafactory/runs/zg1ll0hw
The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. 
The tokenizer class you load from this checkpoint is 'PreTrainedTokenizerFast'. 
The class this function is called from is 'LlamaTokenizer'.
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
Traceback (most recent call last):
  File "/home/ivieira/chicago2/HP_FT_TM/inference_eval.py", line 52, in <module>
    tokenizer = LlamaTokenizer.from_pretrained(model_dir, use_fast=False)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivieira/miniconda3/envs/lf-env/lib/python3.11/site-packages/transformers/tokenization_utils_base.py", line 2213, in from_pretrained
    return cls._from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivieira/miniconda3/envs/lf-env/lib/python3.11/site-packages/transformers/tokenization_utils_base.py", line 2447, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivieira/miniconda3/envs/lf-env/lib/python3.11/site-packages/transformers/models/llama/tokenization_llama.py", line 169, in __init__
    self.sp_model = self.get_spm_processor(kwargs.pop("from_slow", False))
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivieira/miniconda3/envs/lf-env/lib/python3.11/site-packages/transformers/models/llama/tokenization_llama.py", line 196, in get_spm_processor
    tokenizer.Load(self.vocab_file)
  File "/home/ivieira/miniconda3/envs/lf-env/lib/python3.11/site-packages/sentencepiece/__init__.py", line 961, in Load
    return self.LoadFromFile(model_file)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivieira/miniconda3/envs/lf-env/lib/python3.11/site-packages/sentencepiece/__init__.py", line 316, in LoadFromFile
    return _sentencepiece.SentencePieceProcessor_LoadFromFile(self, arg)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: not a string
