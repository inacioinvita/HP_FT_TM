CUDA available? True
Device count: 1
[INFO|2025-01-14 17:27:51] llamafactory.hparams.parser:373 >> Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: torch.bfloat16
[INFO|2025-01-14 17:28:03] llamafactory.data.template:157 >> Add pad token: <|eot_id|>
[INFO|2025-01-14 17:28:03] llamafactory.data.template:157 >> Add <|eot_id|>,<|eom_id|> to stop words.
[INFO|2025-01-14 17:28:03] llamafactory.data.loader:157 >> Loading dataset BALS_de_train_dataset.json...
training example:
input_ids:
[128000, 128006, 9125, 128007, 271, 2675, 527, 264, 11190, 15592, 18328, 369, 14807, 505, 6498, 311, 6063, 13, 1472, 28832, 4320, 449, 279, 2768, 4823, 13155, 25, 5324, 3129, 3332, 928, 9388, 128009, 128006, 882, 128007, 271, 5618, 15025, 279, 2768, 1495, 505, 6498, 311, 6063, 627, 37, 7618, 18977, 50, 128009, 128006, 78191, 128007, 271, 11673, 52, 643, 40451, 42, 320, 37, 7618, 18977, 50, 8, 128009]
inputs:
<|begin_of_text|><|start_header_id|>system<|end_header_id|>

You are a helpful AI assistant for translation from English to German. You MUST answer with the following JSON scheme: {"translation":"string"}<|eot_id|><|start_header_id|>user<|end_header_id|>

Please translate the following text from English to German.
FIREWORKS<|eot_id|><|start_header_id|>assistant<|end_header_id|>

FEUERWERK (FIREWORKS)<|eot_id|>
label_ids:
[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 11673, 52, 643, 40451, 42, 320, 37, 7618, 18977, 50, 8, 128009]
labels:
FEUERWERK (FIREWORKS)<|eot_id|>
[INFO|2025-01-14 17:28:04] llamafactory.model.model_utils.quantization:157 >> Quantizing model to 4 bit with bitsandbytes.
[INFO|2025-01-14 17:28:11] llamafactory.model.model_utils.checkpointing:157 >> Upcasting layernorm weights in float32.
[INFO|2025-01-14 17:28:11] llamafactory.model.model_utils.checkpointing:157 >> Gradient checkpointing enabled.
[INFO|2025-01-14 17:28:11] llamafactory.model.model_utils.attention:157 >> Using torch SDPA for faster training and inference.
[INFO|2025-01-14 17:28:11] llamafactory.model.adapter:157 >> Upcasting trainable params to float32.
[INFO|2025-01-14 17:28:12] llamafactory.model.adapter:157 >> Fine-tuning method: LoRA
[INFO|2025-01-14 17:28:12] llamafactory.model.model_utils.misc:157 >> Found linear modules: v_proj,k_proj,down_proj,up_proj,q_proj,o_proj,gate_proj
[INFO|2025-01-14 17:28:12] llamafactory.model.loader:157 >> trainable params: 41,943,040 || all params: 8,072,204,288 || trainable%: 0.5196
{'loss': 1.3206, 'grad_norm': 0.9054064750671387, 'learning_rate': 0.000993085062240664, 'epoch': 0.01}
{'eval_loss': 1.1114544868469238, 'eval_runtime': 142.9203, 'eval_samples_per_second': 18.094, 'eval_steps_per_second': 1.133, 'epoch': 0.01}
{'loss': 1.1512, 'grad_norm': 0.8697108626365662, 'learning_rate': 0.0009792551867219917, 'epoch': 0.03}
{'eval_loss': 1.0656473636627197, 'eval_runtime': 72.5311, 'eval_samples_per_second': 35.654, 'eval_steps_per_second': 2.234, 'epoch': 0.03}
{'loss': 1.0019, 'grad_norm': 0.9281575679779053, 'learning_rate': 0.0009654253112033194, 'epoch': 0.04}
