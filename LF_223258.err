[INFO|configuration_utils.py:679] 2025-01-14 17:36:50,337 >> loading configuration file config.json from cache at /home/ivieira/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659/config.json
[INFO|configuration_utils.py:746] 2025-01-14 17:36:50,338 >> Model config LlamaConfig {
  "_name_or_path": "meta-llama/Meta-Llama-3.1-8B-Instruct",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.1",
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|tokenization_utils_base.py:2211] 2025-01-14 17:36:50,534 >> loading file tokenizer.json from cache at /home/ivieira/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659/tokenizer.json
[INFO|tokenization_utils_base.py:2211] 2025-01-14 17:36:50,534 >> loading file tokenizer.model from cache at None
[INFO|tokenization_utils_base.py:2211] 2025-01-14 17:36:50,534 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2211] 2025-01-14 17:36:50,534 >> loading file special_tokens_map.json from cache at /home/ivieira/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659/special_tokens_map.json
[INFO|tokenization_utils_base.py:2211] 2025-01-14 17:36:50,534 >> loading file tokenizer_config.json from cache at /home/ivieira/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659/tokenizer_config.json
[INFO|tokenization_utils_base.py:2475] 2025-01-14 17:36:50,901 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:679] 2025-01-14 17:36:51,831 >> loading configuration file config.json from cache at /home/ivieira/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659/config.json
[INFO|configuration_utils.py:746] 2025-01-14 17:36:51,833 >> Model config LlamaConfig {
  "_name_or_path": "meta-llama/Meta-Llama-3.1-8B-Instruct",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.1",
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|tokenization_utils_base.py:2211] 2025-01-14 17:36:52,038 >> loading file tokenizer.json from cache at /home/ivieira/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659/tokenizer.json
[INFO|tokenization_utils_base.py:2211] 2025-01-14 17:36:52,038 >> loading file tokenizer.model from cache at None
[INFO|tokenization_utils_base.py:2211] 2025-01-14 17:36:52,038 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2211] 2025-01-14 17:36:52,039 >> loading file special_tokens_map.json from cache at /home/ivieira/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659/special_tokens_map.json
[INFO|tokenization_utils_base.py:2211] 2025-01-14 17:36:52,039 >> loading file tokenizer_config.json from cache at /home/ivieira/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659/tokenizer_config.json
[INFO|tokenization_utils_base.py:2475] 2025-01-14 17:36:52,411 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:679] 2025-01-14 17:36:53,173 >> loading configuration file config.json from cache at /home/ivieira/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659/config.json
[INFO|configuration_utils.py:746] 2025-01-14 17:36:53,173 >> Model config LlamaConfig {
  "_name_or_path": "meta-llama/Meta-Llama-3.1-8B-Instruct",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.1",
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|modeling_utils.py:3937] 2025-01-14 17:36:53,250 >> loading weights file model.safetensors from cache at /home/ivieira/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659/model.safetensors.index.json
[INFO|modeling_utils.py:1670] 2025-01-14 17:36:53,251 >> Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.
[INFO|configuration_utils.py:1096] 2025-01-14 17:36:53,252 >> Generate config GenerationConfig {
  "bos_token_id": 128000,
  "eos_token_id": [
    128001,
    128008,
    128009
  ]
}

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|‚ñà‚ñà‚ñå       | 1/4 [00:02<00:08,  2.76s/it]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 2/4 [00:05<00:05,  2.64s/it]Loading checkpoint shards:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 3/4 [00:07<00:02,  2.61s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:08<00:00,  1.79s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:08<00:00,  2.11s/it]
[INFO|modeling_utils.py:4800] 2025-01-14 17:37:01,811 >> All model checkpoint weights were used when initializing LlamaForCausalLM.

[INFO|modeling_utils.py:4808] 2025-01-14 17:37:01,811 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at meta-llama/Meta-Llama-3.1-8B-Instruct.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
[INFO|configuration_utils.py:1051] 2025-01-14 17:37:02,023 >> loading configuration file generation_config.json from cache at /home/ivieira/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659/generation_config.json
[INFO|configuration_utils.py:1096] 2025-01-14 17:37:02,024 >> Generate config GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "temperature": 0.6,
  "top_p": 0.9
}

[INFO|trainer.py:698] 2025-01-14 17:37:03,984 >> Using auto half precision backend
[INFO|trainer.py:2313] 2025-01-14 17:37:04,211 >> ***** Running training *****
[INFO|trainer.py:2314] 2025-01-14 17:37:04,211 >>   Num examples = 23,270
[INFO|trainer.py:2315] 2025-01-14 17:37:04,211 >>   Num Epochs = 1
[INFO|trainer.py:2316] 2025-01-14 17:37:04,211 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:2319] 2025-01-14 17:37:04,211 >>   Total train batch size (w. parallel, distributed & accumulation) = 256
[INFO|trainer.py:2320] 2025-01-14 17:37:04,211 >>   Gradient Accumulation steps = 8
[INFO|trainer.py:2321] 2025-01-14 17:37:04,211 >>   Total optimization steps = 1
[INFO|trainer.py:2322] 2025-01-14 17:37:04,214 >>   Number of trainable parameters = 167,772,160
[INFO|integration_utils.py:812] 2025-01-14 17:37:04,217 >> Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
wandb: Currently logged in as: inaciovieira (inaciovieira-alpha-crc). Use `wandb login --relogin` to force relogin
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.2
wandb: Run data is saved locally in /home/ivieira/LLaMA-Factory/wandb/run-20250114_173704-qyly5pzh
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run train_2025-01-14-17-36-44
wandb: ‚≠êÔ∏è View project at https://wandb.ai/inaciovieira-alpha-crc/llamafactory
wandb: üöÄ View run at https://wandb.ai/inaciovieira-alpha-crc/llamafactory/runs/qyly5pzh
  0%|          | 0/1 [00:00<?, ?it/s]/home/ivieira/miniconda3/envs/lf-env/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/home/ivieira/miniconda3/envs/lf-env/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:49<00:00, 49.53s/it][INFO|trainer.py:3801] 2025-01-14 17:37:54,903 >> Saving model checkpoint to saves/Llama-3.1-8B-Instruct/lora/train_2025-01-14-17-36-44/checkpoint-1
[INFO|configuration_utils.py:679] 2025-01-14 17:37:55,339 >> loading configuration file config.json from cache at /home/ivieira/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659/config.json
[INFO|configuration_utils.py:746] 2025-01-14 17:37:55,340 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.1",
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|tokenization_utils_base.py:2646] 2025-01-14 17:37:55,945 >> tokenizer config file saved in saves/Llama-3.1-8B-Instruct/lora/train_2025-01-14-17-36-44/checkpoint-1/tokenizer_config.json
[INFO|tokenization_utils_base.py:2655] 2025-01-14 17:37:55,946 >> Special tokens file saved in saves/Llama-3.1-8B-Instruct/lora/train_2025-01-14-17-36-44/checkpoint-1/special_tokens_map.json
[INFO|trainer.py:2584] 2025-01-14 17:37:57,058 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                             100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:51<00:00, 49.53s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:51<00:00, 51.71s/it]
[INFO|trainer.py:3801] 2025-01-14 17:37:57,077 >> Saving model checkpoint to saves/Llama-3.1-8B-Instruct/lora/train_2025-01-14-17-36-44
[INFO|configuration_utils.py:679] 2025-01-14 17:37:57,536 >> loading configuration file config.json from cache at /home/ivieira/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659/config.json
[INFO|configuration_utils.py:746] 2025-01-14 17:37:57,537 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.1",
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|tokenization_utils_base.py:2646] 2025-01-14 17:37:58,134 >> tokenizer config file saved in saves/Llama-3.1-8B-Instruct/lora/train_2025-01-14-17-36-44/tokenizer_config.json
[INFO|tokenization_utils_base.py:2655] 2025-01-14 17:37:58,135 >> Special tokens file saved in saves/Llama-3.1-8B-Instruct/lora/train_2025-01-14-17-36-44/special_tokens_map.json
[INFO|trainer.py:4117] 2025-01-14 17:37:58,321 >> 
***** Running Evaluation *****
[INFO|trainer.py:4119] 2025-01-14 17:37:58,321 >>   Num examples = 2586
[INFO|trainer.py:4122] 2025-01-14 17:37:58,322 >>   Batch size = 16
/home/ivieira/miniconda3/envs/lf-env/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/home/ivieira/miniconda3/envs/lf-env/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
  0%|          | 0/162 [00:00<?, ?it/s]  1%|          | 2/162 [00:00<00:35,  4.51it/s]  2%|‚ñè         | 3/162 [00:00<00:47,  3.32it/s]  2%|‚ñè         | 4/162 [00:01<00:56,  2.82it/s]  3%|‚ñé         | 5/162 [00:01<01:07,  2.32it/s]  4%|‚ñé         | 6/162 [00:02<01:05,  2.39it/s]  4%|‚ñç         | 7/162 [00:02<01:05,  2.38it/s]  5%|‚ñç         | 8/162 [00:03<01:12,  2.14it/s]  6%|‚ñå         | 9/162 [00:03<01:09,  2.19it/s]  6%|‚ñå         | 10/162 [00:04<01:16,  1.98it/s]  7%|‚ñã         | 11/162 [00:05<01:30,  1.67it/s]  7%|‚ñã         | 12/162 [00:05<01:20,  1.86it/s]  8%|‚ñä         | 13/162 [00:06<01:20,  1.86it/s]  9%|‚ñä         | 14/162 [00:06<01:15,  1.96it/s]  9%|‚ñâ         | 15/162 [00:06<01:12,  2.03it/s] 10%|‚ñâ         | 16/162 [00:07<01:10,  2.07it/s] 10%|‚ñà         | 17/162 [00:07<01:10,  2.05it/s] 11%|‚ñà         | 18/162 [00:08<01:14,  1.93it/s] 12%|‚ñà‚ñè        | 19/162 [00:08<01:11,  1.99it/s] 12%|‚ñà‚ñè        | 20/162 [00:09<01:16,  1.86it/s] 13%|‚ñà‚ñé        | 21/162 [00:10<01:11,  1.96it/s] 14%|‚ñà‚ñé        | 22/162 [00:10<01:06,  2.11it/s] 14%|‚ñà‚ñç        | 23/162 [00:11<01:16,  1.81it/s] 15%|‚ñà‚ñç        | 24/162 [00:11<01:11,  1.93it/s] 15%|‚ñà‚ñå        | 25/162 [00:12<01:08,  1.99it/s] 16%|‚ñà‚ñå        | 26/162 [00:12<01:05,  2.06it/s] 17%|‚ñà‚ñã        | 27/162 [00:13<01:43,  1.31it/s] 17%|‚ñà‚ñã        | 28/162 [00:14<01:29,  1.50it/s] 18%|‚ñà‚ñä        | 29/162 [00:14<01:18,  1.69it/s] 19%|‚ñà‚ñä        | 30/162 [00:15<01:14,  1.77it/s] 19%|‚ñà‚ñâ        | 31/162 [00:15<01:09,  1.89it/s] 20%|‚ñà‚ñâ        | 32/162 [00:16<01:04,  2.02it/s] 20%|‚ñà‚ñà        | 33/162 [00:16<01:01,  2.11it/s] 21%|‚ñà‚ñà        | 34/162 [00:17<01:08,  1.86it/s] 22%|‚ñà‚ñà‚ñè       | 35/162 [00:17<01:04,  1.98it/s] 22%|‚ñà‚ñà‚ñè       | 36/162 [00:19<01:42,  1.23it/s] 23%|‚ñà‚ñà‚ñé       | 37/162 [00:19<01:27,  1.43it/s] 23%|‚ñà‚ñà‚ñé       | 38/162 [00:20<01:17,  1.59it/s] 24%|‚ñà‚ñà‚ñç       | 39/162 [00:20<01:18,  1.57it/s] 25%|‚ñà‚ñà‚ñç       | 40/162 [00:21<01:07,  1.80it/s] 25%|‚ñà‚ñà‚ñå       | 41/162 [00:21<01:07,  1.78it/s] 26%|‚ñà‚ñà‚ñå       | 42/162 [00:22<01:00,  2.00it/s] 27%|‚ñà‚ñà‚ñã       | 43/162 [00:22<00:57,  2.07it/s] 27%|‚ñà‚ñà‚ñã       | 44/162 [00:23<01:03,  1.86it/s] 28%|‚ñà‚ñà‚ñä       | 45/162 [00:23<00:58,  1.98it/s] 28%|‚ñà‚ñà‚ñä       | 46/162 [00:24<00:55,  2.08it/s] 29%|‚ñà‚ñà‚ñâ       | 47/162 [00:24<00:55,  2.06it/s] 30%|‚ñà‚ñà‚ñâ       | 48/162 [00:25<00:55,  2.05it/s] 30%|‚ñà‚ñà‚ñà       | 49/162 [00:25<00:52,  2.15it/s] 31%|‚ñà‚ñà‚ñà       | 50/162 [00:25<00:52,  2.12it/s] 31%|‚ñà‚ñà‚ñà‚ñè      | 51/162 [00:26<00:50,  2.20it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 52/162 [00:26<00:49,  2.22it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 53/162 [00:27<00:47,  2.31it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 54/162 [00:27<00:46,  2.32it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 55/162 [00:27<00:44,  2.41it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 56/162 [00:28<00:43,  2.42it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 57/162 [00:28<00:46,  2.28it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 58/162 [00:29<00:45,  2.30it/s] 36%|‚ñà‚ñà‚ñà‚ñã      | 59/162 [00:29<00:48,  2.14it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 60/162 [00:30<00:47,  2.15it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 61/162 [00:30<00:46,  2.18it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 62/162 [00:31<00:44,  2.22it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 63/162 [00:31<00:43,  2.26it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 64/162 [00:32<00:45,  2.16it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 65/162 [00:32<00:44,  2.17it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 66/162 [00:32<00:42,  2.24it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 67/162 [00:33<00:43,  2.17it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 68/162 [00:33<00:40,  2.30it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 69/162 [00:34<00:42,  2.19it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 70/162 [00:34<00:41,  2.21it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 71/162 [00:35<00:40,  2.27it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 72/162 [00:36<00:51,  1.73it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 73/162 [00:36<00:47,  1.88it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 74/162 [00:36<00:44,  1.98it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 75/162 [00:37<00:40,  2.12it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 76/162 [00:37<00:43,  1.99it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 77/162 [00:38<00:45,  1.87it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 78/162 [00:40<01:21,  1.03it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 79/162 [00:40<01:06,  1.25it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 80/162 [00:41<00:58,  1.40it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 81/162 [00:41<00:51,  1.57it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 82/162 [00:42<00:45,  1.74it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 83/162 [00:42<00:41,  1.92it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 84/162 [00:43<00:38,  2.05it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 85/162 [00:43<00:35,  2.20it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 86/162 [00:44<00:35,  2.16it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 87/162 [00:44<00:42,  1.76it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 88/162 [00:45<00:39,  1.87it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 89/162 [00:45<00:35,  2.03it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 90/162 [00:46<00:35,  2.03it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 91/162 [00:46<00:33,  2.09it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 92/162 [00:47<00:32,  2.18it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 93/162 [00:47<00:30,  2.29it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 94/162 [00:47<00:29,  2.30it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 95/162 [00:48<00:28,  2.37it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 96/162 [00:48<00:28,  2.31it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 97/162 [00:49<00:28,  2.27it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 98/162 [00:49<00:27,  2.32it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 99/162 [00:49<00:26,  2.41it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 100/162 [00:50<00:26,  2.30it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 101/162 [00:50<00:26,  2.31it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 102/162 [00:51<00:25,  2.38it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 103/162 [00:51<00:26,  2.19it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 104/162 [00:52<00:25,  2.25it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 105/162 [00:52<00:25,  2.23it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 106/162 [00:53<00:23,  2.35it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 107/162 [00:53<00:22,  2.41it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 108/162 [00:53<00:22,  2.41it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 109/162 [00:54<00:22,  2.39it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 110/162 [00:54<00:21,  2.40it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 111/162 [00:55<00:20,  2.44it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 112/162 [00:55<00:21,  2.36it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 113/162 [00:55<00:20,  2.42it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 114/162 [00:56<00:19,  2.48it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 115/162 [00:56<00:19,  2.43it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 116/162 [00:57<00:18,  2.52it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 117/162 [00:57<00:18,  2.44it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 118/162 [00:57<00:18,  2.35it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 119/162 [00:58<00:19,  2.25it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 120/162 [00:58<00:18,  2.29it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 121/162 [00:59<00:18,  2.21it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 122/162 [00:59<00:18,  2.20it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 123/162 [01:00<00:17,  2.24it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 124/162 [01:00<00:16,  2.24it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 125/162 [01:01<00:16,  2.23it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 126/162 [01:01<00:15,  2.32it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 127/162 [01:01<00:14,  2.38it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 128/162 [01:02<00:14,  2.32it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 129/162 [01:02<00:14,  2.28it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 130/162 [01:03<00:13,  2.38it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 131/162 [01:03<00:14,  2.19it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 132/162 [01:04<00:13,  2.21it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 133/162 [01:04<00:12,  2.25it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 134/162 [01:05<00:12,  2.30it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 135/162 [01:05<00:11,  2.37it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 136/162 [01:05<00:11,  2.28it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 137/162 [01:06<00:11,  2.15it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 138/162 [01:06<00:10,  2.22it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 139/162 [01:07<00:09,  2.31it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 140/162 [01:07<00:09,  2.41it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 141/162 [01:08<00:08,  2.36it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 142/162 [01:08<00:08,  2.33it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 143/162 [01:08<00:08,  2.28it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 144/162 [01:09<00:08,  2.20it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 145/162 [01:09<00:07,  2.24it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 146/162 [01:10<00:06,  2.33it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 147/162 [01:11<00:07,  1.94it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 148/162 [01:11<00:06,  2.02it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 149/162 [01:11<00:06,  2.16it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 150/162 [01:12<00:05,  2.19it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 151/162 [01:12<00:04,  2.23it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 152/162 [01:13<00:04,  2.10it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 153/162 [01:13<00:04,  2.17it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 154/162 [01:14<00:03,  2.24it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 155/162 [01:14<00:03,  2.19it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 156/162 [01:14<00:02,  2.32it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 157/162 [01:15<00:02,  2.02it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 158/162 [01:15<00:01,  2.15it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 159/162 [01:16<00:01,  2.21it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 160/162 [01:16<00:00,  2.24it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 161/162 [01:17<00:00,  2.20it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 162/162 [01:17<00:00,  2.46it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 162/162 [01:17<00:00,  2.09it/s]
[INFO|modelcard.py:449] 2025-01-14 17:39:16,419 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}
wandb: Currently logged in as: inaciovieira (inaciovieira-alpha-crc). Use `wandb login --relogin` to force relogin
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.2
wandb: Run data is saved locally in /home/ivieira/LLaMA-Factory/wandb/run-20250114_173924-qyly5pzh
wandb: Run `wandb offline` to turn off syncing.
wandb: Resuming run train_2025-01-14-17-36-44
wandb: ‚≠êÔ∏è View project at https://wandb.ai/inaciovieira-alpha-crc/llamafactory
wandb: üöÄ View run at https://wandb.ai/inaciovieira-alpha-crc/llamafactory/runs/qyly5pzh
We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|‚ñà‚ñà‚ñå       | 1/4 [00:01<00:05,  1.70s/it]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 2/4 [00:03<00:03,  1.72s/it]Loading checkpoint shards:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 3/4 [00:05<00:01,  1.70s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:05<00:00,  1.22s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:05<00:00,  1.40s/it]
/home/ivieira/miniconda3/envs/lf-env/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/ivieira/miniconda3/envs/lf-env/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]Fetching 5 files:  40%|‚ñà‚ñà‚ñà‚ñà      | 2/5 [00:44<01:06, 22.24s/it]Fetching 5 files:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 4/5 [00:53<00:11, 11.84s/it]Fetching 5 files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:53<00:00, 10.72s/it]
Lightning automatically upgraded your loaded checkpoint from v1.3.5 to v2.5.0.post0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../.cache/huggingface/hub/models--Unbabel--wmt20-comet-da/snapshots/87819f4d6d4f17e0d1752cc9e0ccfa2064997219/checkpoints/model.ckpt`
Encoder model frozen.
/home/ivieira/miniconda3/envs/lf-env/lib/python3.11/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['encoder.model.embeddings.position_ids']
/home/ivieira/miniconda3/envs/lf-env/lib/python3.11/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/ivieira/chicago2/HP_FT_TM/inference_eval.py -- ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
You are using a CUDA device ('NVIDIA A100-SXM4-80GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Predicting: 0it [00:00, ?it/s]Predicting:   0%|          | 0/360 [00:00<?, ?it/s]Predicting DataLoader 0:   0%|          | 0/360 [00:00<?, ?it/s]Predicting DataLoader 0:   0%|          | 1/360 [00:00<00:36,  9.82it/s]Predicting DataLoader 0:   1%|          | 2/360 [00:00<00:25, 14.13it/s]Predicting DataLoader 0:   1%|          | 3/360 [00:00<00:21, 16.47it/s]Predicting DataLoader 0:   1%|          | 4/360 [00:00<00:19, 18.24it/s]Predicting DataLoader 0:   1%|‚ñè         | 5/360 [00:00<00:18, 19.17it/s]Predicting DataLoader 0:   2%|‚ñè         | 6/360 [00:00<00:17, 19.94it/s]Predicting DataLoader 0:   2%|‚ñè         | 7/360 [00:00<00:17, 20.50it/s]Predicting DataLoader 0:   2%|‚ñè         | 8/360 [00:00<00:16, 21.02it/s]Predicting DataLoader 0:   2%|‚ñé         | 9/360 [00:00<00:16, 21.40it/s]Predicting DataLoader 0:   3%|‚ñé         | 10/360 [00:00<00:16, 21.72it/s]Predicting DataLoader 0:   3%|‚ñé         | 11/360 [00:00<00:15, 21.97it/s]Predicting DataLoader 0:   3%|‚ñé         | 12/360 [00:00<00:15, 22.20it/s]Predicting DataLoader 0:   4%|‚ñé         | 13/360 [00:00<00:15, 22.41it/s]Predicting DataLoader 0:   4%|‚ñç         | 14/360 [00:00<00:15, 22.62it/s]Predicting DataLoader 0:   4%|‚ñç         | 15/360 [00:00<00:15, 22.76it/s]Predicting DataLoader 0:   4%|‚ñç         | 16/360 [00:00<00:15, 22.91it/s]Predicting DataLoader 0:   5%|‚ñç         | 17/360 [00:00<00:14, 23.02it/s]Predicting DataLoader 0:   5%|‚ñå         | 18/360 [00:00<00:14, 23.20it/s]Predicting DataLoader 0:   5%|‚ñå         | 19/360 [00:00<00:14, 23.29it/s]Predicting DataLoader 0:   6%|‚ñå         | 20/360 [00:00<00:14, 23.37it/s]Predicting DataLoader 0:   6%|‚ñå         | 21/360 [00:00<00:14, 23.46it/s]Predicting DataLoader 0:   6%|‚ñå         | 22/360 [00:00<00:14, 23.57it/s]Predicting DataLoader 0:   6%|‚ñã         | 23/360 [00:00<00:14, 23.64it/s]Predicting DataLoader 0:   7%|‚ñã         | 24/360 [00:01<00:14, 23.74it/s]Predicting DataLoader 0:   7%|‚ñã         | 25/360 [00:01<00:14, 23.77it/s]Predicting DataLoader 0:   7%|‚ñã         | 26/360 [00:01<00:14, 23.85it/s]Predicting DataLoader 0:   8%|‚ñä         | 27/360 [00:01<00:13, 23.92it/s]Predicting DataLoader 0:   8%|‚ñä         | 28/360 [00:01<00:13, 24.02it/s]Predicting DataLoader 0:   8%|‚ñä         | 29/360 [00:01<00:13, 24.06it/s]Predicting DataLoader 0:   8%|‚ñä         | 30/360 [00:01<00:13, 24.11it/s]Predicting DataLoader 0:   9%|‚ñä         | 31/360 [00:01<00:13, 24.14it/s]Predicting DataLoader 0:   9%|‚ñâ         | 32/360 [00:01<00:13, 24.19it/s]Predicting DataLoader 0:   9%|‚ñâ         | 33/360 [00:01<00:13, 24.25it/s]Predicting DataLoader 0:   9%|‚ñâ         | 34/360 [00:01<00:13, 24.30it/s]Predicting DataLoader 0:  10%|‚ñâ         | 35/360 [00:01<00:13, 24.32it/s]Predicting DataLoader 0:  10%|‚ñà         | 36/360 [00:01<00:13, 24.37it/s]Predicting DataLoader 0:  10%|‚ñà         | 37/360 [00:01<00:13, 24.37it/s]Predicting DataLoader 0:  11%|‚ñà         | 38/360 [00:01<00:13, 24.40it/s]Predicting DataLoader 0:  11%|‚ñà         | 39/360 [00:01<00:13, 24.40it/s]Predicting DataLoader 0:  11%|‚ñà         | 40/360 [00:01<00:13, 24.41it/s]Predicting DataLoader 0:  11%|‚ñà‚ñè        | 41/360 [00:01<00:13, 24.40it/s]Predicting DataLoader 0:  12%|‚ñà‚ñè        | 42/360 [00:01<00:13, 24.41it/s]Predicting DataLoader 0:  12%|‚ñà‚ñè        | 43/360 [00:01<00:13, 24.34it/s]Predicting DataLoader 0:  12%|‚ñà‚ñè        | 44/360 [00:01<00:12, 24.37it/s]Predicting DataLoader 0:  12%|‚ñà‚ñé        | 45/360 [00:01<00:12, 24.39it/s]Predicting DataLoader 0:  13%|‚ñà‚ñé        | 46/360 [00:01<00:12, 24.42it/s]Predicting DataLoader 0:  13%|‚ñà‚ñé        | 47/360 [00:01<00:12, 24.44it/s]Predicting DataLoader 0:  13%|‚ñà‚ñé        | 48/360 [00:01<00:12, 24.44it/s]Predicting DataLoader 0:  14%|‚ñà‚ñé        | 49/360 [00:02<00:12, 24.45it/s]Predicting DataLoader 0:  14%|‚ñà‚ñç        | 50/360 [00:02<00:12, 24.47it/s]Predicting DataLoader 0:  14%|‚ñà‚ñç        | 51/360 [00:02<00:12, 24.48it/s]Predicting DataLoader 0:  14%|‚ñà‚ñç        | 52/360 [00:02<00:12, 24.50it/s]Predicting DataLoader 0:  15%|‚ñà‚ñç        | 53/360 [00:02<00:12, 24.50it/s]Predicting DataLoader 0:  15%|‚ñà‚ñå        | 54/360 [00:02<00:12, 24.54it/s]Predicting DataLoader 0:  15%|‚ñà‚ñå        | 55/360 [00:02<00:12, 24.56it/s]Predicting DataLoader 0:  16%|‚ñà‚ñå        | 56/360 [00:02<00:12, 24.57it/s]Predicting DataLoader 0:  16%|‚ñà‚ñå        | 57/360 [00:02<00:12, 24.59it/s]Predicting DataLoader 0:  16%|‚ñà‚ñå        | 58/360 [00:02<00:12, 24.59it/s]Predicting DataLoader 0:  16%|‚ñà‚ñã        | 59/360 [00:02<00:12, 24.62it/s]Predicting DataLoader 0:  17%|‚ñà‚ñã        | 60/360 [00:02<00:12, 24.62it/s]Predicting DataLoader 0:  17%|‚ñà‚ñã        | 61/360 [00:02<00:12, 24.65it/s]Predicting DataLoader 0:  17%|‚ñà‚ñã        | 62/360 [00:02<00:12, 24.66it/s]Predicting DataLoader 0:  18%|‚ñà‚ñä        | 63/360 [00:02<00:12, 24.68it/s]Predicting DataLoader 0:  18%|‚ñà‚ñä        | 64/360 [00:02<00:11, 24.69it/s]Predicting DataLoader 0:  18%|‚ñà‚ñä        | 65/360 [00:02<00:11, 24.71it/s]Predicting DataLoader 0:  18%|‚ñà‚ñä        | 66/360 [00:02<00:11, 24.72it/s]Predicting DataLoader 0:  19%|‚ñà‚ñä        | 67/360 [00:02<00:11, 24.74it/s]Predicting DataLoader 0:  19%|‚ñà‚ñâ        | 68/360 [00:02<00:11, 24.74it/s]Predicting DataLoader 0:  19%|‚ñà‚ñâ        | 69/360 [00:02<00:11, 24.76it/s]Predicting DataLoader 0:  19%|‚ñà‚ñâ        | 70/360 [00:02<00:11, 24.76it/s]Predicting DataLoader 0:  20%|‚ñà‚ñâ        | 71/360 [00:02<00:11, 24.78it/s]Predicting DataLoader 0:  20%|‚ñà‚ñà        | 72/360 [00:02<00:11, 24.78it/s]Predicting DataLoader 0:  20%|‚ñà‚ñà        | 73/360 [00:02<00:11, 24.79it/s]Predicting DataLoader 0:  21%|‚ñà‚ñà        | 74/360 [00:02<00:11, 24.79it/s]Predicting DataLoader 0:  21%|‚ñà‚ñà        | 75/360 [00:03<00:11, 24.80it/s]Predicting DataLoader 0:  21%|‚ñà‚ñà        | 76/360 [00:03<00:11, 24.80it/s]Predicting DataLoader 0:  21%|‚ñà‚ñà‚ñè       | 77/360 [00:03<00:11, 24.81it/s]Predicting DataLoader 0:  22%|‚ñà‚ñà‚ñè       | 78/360 [00:03<00:11, 24.81it/s]Predicting DataLoader 0:  22%|‚ñà‚ñà‚ñè       | 79/360 [00:03<00:11, 24.76it/s]Predicting DataLoader 0:  22%|‚ñà‚ñà‚ñè       | 80/360 [00:03<00:11, 24.77it/s]Predicting DataLoader 0:  22%|‚ñà‚ñà‚ñé       | 81/360 [00:03<00:11, 24.78it/s]Predicting DataLoader 0:  23%|‚ñà‚ñà‚ñé       | 82/360 [00:03<00:11, 24.79it/s]Predicting DataLoader 0:  23%|‚ñà‚ñà‚ñé       | 83/360 [00:03<00:11, 24.79it/s]Predicting DataLoader 0:  23%|‚ñà‚ñà‚ñé       | 84/360 [00:03<00:11, 24.80it/s]Predicting DataLoader 0:  24%|‚ñà‚ñà‚ñé       | 85/360 [00:03<00:11, 24.80it/s]Predicting DataLoader 0:  24%|‚ñà‚ñà‚ñç       | 86/360 [00:03<00:11, 24.82it/s]Predicting DataLoader 0:  24%|‚ñà‚ñà‚ñç       | 87/360 [00:03<00:11, 24.79it/s]Predicting DataLoader 0:  24%|‚ñà‚ñà‚ñç       | 88/360 [00:03<00:10, 24.80it/s]Predicting DataLoader 0:  25%|‚ñà‚ñà‚ñç       | 89/360 [00:03<00:10, 24.81it/s]Predicting DataLoader 0:  25%|‚ñà‚ñà‚ñå       | 90/360 [00:03<00:10, 24.82it/s]Predicting DataLoader 0:  25%|‚ñà‚ñà‚ñå       | 91/360 [00:03<00:10, 24.81it/s]Predicting DataLoader 0:  26%|‚ñà‚ñà‚ñå       | 92/360 [00:03<00:10, 24.82it/s]Predicting DataLoader 0:  26%|‚ñà‚ñà‚ñå       | 93/360 [00:03<00:10, 24.82it/s]Predicting DataLoader 0:  26%|‚ñà‚ñà‚ñå       | 94/360 [00:03<00:10, 24.82it/s]Predicting DataLoader 0:  26%|‚ñà‚ñà‚ñã       | 95/360 [00:03<00:10, 24.82it/s]Predicting DataLoader 0:  27%|‚ñà‚ñà‚ñã       | 96/360 [00:03<00:10, 24.82it/s]Predicting DataLoader 0:  27%|‚ñà‚ñà‚ñã       | 97/360 [00:03<00:10, 24.82it/s]Predicting DataLoader 0:  27%|‚ñà‚ñà‚ñã       | 98/360 [00:03<00:10, 24.83it/s]Predicting DataLoader 0:  28%|‚ñà‚ñà‚ñä       | 99/360 [00:03<00:10, 24.84it/s]Predicting DataLoader 0:  28%|‚ñà‚ñà‚ñä       | 100/360 [00:04<00:10, 24.84it/s]Predicting DataLoader 0:  28%|‚ñà‚ñà‚ñä       | 101/360 [00:04<00:10, 24.85it/s]Predicting DataLoader 0:  28%|‚ñà‚ñà‚ñä       | 102/360 [00:04<00:10, 24.85it/s]Predicting DataLoader 0:  29%|‚ñà‚ñà‚ñä       | 103/360 [00:04<00:10, 24.85it/s]Predicting DataLoader 0:  29%|‚ñà‚ñà‚ñâ       | 104/360 [00:04<00:10, 24.85it/s]Predicting DataLoader 0:  29%|‚ñà‚ñà‚ñâ       | 105/360 [00:04<00:10, 24.85it/s]Predicting DataLoader 0:  29%|‚ñà‚ñà‚ñâ       | 106/360 [00:04<00:10, 24.86it/s]Predicting DataLoader 0:  30%|‚ñà‚ñà‚ñâ       | 107/360 [00:04<00:10, 24.86it/s]Predicting DataLoader 0:  30%|‚ñà‚ñà‚ñà       | 108/360 [00:04<00:10, 24.85it/s]Predicting DataLoader 0:  30%|‚ñà‚ñà‚ñà       | 109/360 [00:04<00:10, 24.85it/s]Predicting DataLoader 0:  31%|‚ñà‚ñà‚ñà       | 110/360 [00:04<00:10, 24.86it/s]Predicting DataLoader 0:  31%|‚ñà‚ñà‚ñà       | 111/360 [00:04<00:10, 24.86it/s]Predicting DataLoader 0:  31%|‚ñà‚ñà‚ñà       | 112/360 [00:04<00:09, 24.85it/s]Predicting DataLoader 0:  31%|‚ñà‚ñà‚ñà‚ñè      | 113/360 [00:04<00:09, 24.85it/s]Predicting DataLoader 0:  32%|‚ñà‚ñà‚ñà‚ñè      | 114/360 [00:04<00:09, 24.85it/s]Predicting DataLoader 0:  32%|‚ñà‚ñà‚ñà‚ñè      | 115/360 [00:04<00:09, 24.84it/s]Predicting DataLoader 0:  32%|‚ñà‚ñà‚ñà‚ñè      | 116/360 [00:04<00:09, 24.85it/s]Predicting DataLoader 0:  32%|‚ñà‚ñà‚ñà‚ñé      | 117/360 [00:04<00:09, 24.83it/s]Predicting DataLoader 0:  33%|‚ñà‚ñà‚ñà‚ñé      | 118/360 [00:04<00:09, 24.83it/s]Predicting DataLoader 0:  33%|‚ñà‚ñà‚ñà‚ñé      | 119/360 [00:04<00:09, 24.83it/s]Predicting DataLoader 0:  33%|‚ñà‚ñà‚ñà‚ñé      | 120/360 [00:04<00:09, 24.81it/s]Predicting DataLoader 0:  34%|‚ñà‚ñà‚ñà‚ñé      | 121/360 [00:04<00:09, 24.80it/s]Predicting DataLoader 0:  34%|‚ñà‚ñà‚ñà‚ñç      | 122/360 [00:04<00:09, 24.80it/s]Predicting DataLoader 0:  34%|‚ñà‚ñà‚ñà‚ñç      | 123/360 [00:04<00:09, 24.79it/s]Predicting DataLoader 0:  34%|‚ñà‚ñà‚ñà‚ñç      | 124/360 [00:05<00:09, 24.77it/s]Predicting DataLoader 0:  35%|‚ñà‚ñà‚ñà‚ñç      | 125/360 [00:05<00:09, 24.76it/s]Predicting DataLoader 0:  35%|‚ñà‚ñà‚ñà‚ñå      | 126/360 [00:05<00:09, 24.75it/s]Predicting DataLoader 0:  35%|‚ñà‚ñà‚ñà‚ñå      | 127/360 [00:05<00:09, 24.74it/s]Predicting DataLoader 0:  36%|‚ñà‚ñà‚ñà‚ñå      | 128/360 [00:05<00:09, 24.74it/s]Predicting DataLoader 0:  36%|‚ñà‚ñà‚ñà‚ñå      | 129/360 [00:05<00:09, 24.74it/s]Predicting DataLoader 0:  36%|‚ñà‚ñà‚ñà‚ñå      | 130/360 [00:05<00:09, 24.74it/s]Predicting DataLoader 0:  36%|‚ñà‚ñà‚ñà‚ñã      | 131/360 [00:05<00:09, 24.74it/s]Predicting DataLoader 0:  37%|‚ñà‚ñà‚ñà‚ñã      | 132/360 [00:05<00:09, 24.74it/s]Predicting DataLoader 0:  37%|‚ñà‚ñà‚ñà‚ñã      | 133/360 [00:05<00:09, 24.74it/s]Predicting DataLoader 0:  37%|‚ñà‚ñà‚ñà‚ñã      | 134/360 [00:05<00:09, 24.74it/s]Predicting DataLoader 0:  38%|‚ñà‚ñà‚ñà‚ñä      | 135/360 [00:05<00:09, 24.71it/s]Predicting DataLoader 0:  38%|‚ñà‚ñà‚ñà‚ñä      | 136/360 [00:05<00:09, 24.70it/s]Predicting DataLoader 0:  38%|‚ñà‚ñà‚ñà‚ñä      | 137/360 [00:05<00:09, 24.68it/s]Predicting DataLoader 0:  38%|‚ñà‚ñà‚ñà‚ñä      | 138/360 [00:05<00:09, 24.66it/s]Predicting DataLoader 0:  39%|‚ñà‚ñà‚ñà‚ñä      | 139/360 [00:05<00:08, 24.65it/s]Predicting DataLoader 0:  39%|‚ñà‚ñà‚ñà‚ñâ      | 140/360 [00:05<00:08, 24.65it/s]Predicting DataLoader 0:  39%|‚ñà‚ñà‚ñà‚ñâ      | 141/360 [00:05<00:08, 24.65it/s]Predicting DataLoader 0:  39%|‚ñà‚ñà‚ñà‚ñâ      | 142/360 [00:05<00:08, 24.65it/s]Predicting DataLoader 0:  40%|‚ñà‚ñà‚ñà‚ñâ      | 143/360 [00:05<00:08, 24.62it/s]Predicting DataLoader 0:  40%|‚ñà‚ñà‚ñà‚ñà      | 144/360 [00:05<00:08, 24.62it/s]Predicting DataLoader 0:  40%|‚ñà‚ñà‚ñà‚ñà      | 145/360 [00:05<00:08, 24.62it/s]Predicting DataLoader 0:  41%|‚ñà‚ñà‚ñà‚ñà      | 146/360 [00:05<00:08, 24.62it/s]Predicting DataLoader 0:  41%|‚ñà‚ñà‚ñà‚ñà      | 147/360 [00:05<00:08, 24.63it/s]Predicting DataLoader 0:  41%|‚ñà‚ñà‚ñà‚ñà      | 148/360 [00:06<00:08, 24.62it/s]Predicting DataLoader 0:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 149/360 [00:06<00:08, 24.62it/s]Predicting DataLoader 0:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 150/360 [00:06<00:08, 24.61it/s]Predicting DataLoader 0:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 151/360 [00:06<00:08, 24.61it/s]Predicting DataLoader 0:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 152/360 [00:06<00:08, 24.61it/s]Predicting DataLoader 0:  42%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 153/360 [00:06<00:08, 24.61it/s]Predicting DataLoader 0:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 154/360 [00:06<00:08, 24.61it/s]Predicting DataLoader 0:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 155/360 [00:06<00:08, 24.61it/s]Predicting DataLoader 0:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 156/360 [00:06<00:08, 24.60it/s]Predicting DataLoader 0:  44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 157/360 [00:06<00:08, 24.60it/s]Predicting DataLoader 0:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 158/360 [00:06<00:08, 24.60it/s]Predicting DataLoader 0:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 159/360 [00:06<00:08, 24.59it/s]Predicting DataLoader 0:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 160/360 [00:06<00:08, 24.59it/s]Predicting DataLoader 0:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 161/360 [00:06<00:08, 24.59it/s]Predicting DataLoader 0:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 162/360 [00:06<00:08, 24.57it/s]Predicting DataLoader 0:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 163/360 [00:06<00:08, 24.56it/s]Predicting DataLoader 0:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 164/360 [00:06<00:07, 24.56it/s]Predicting DataLoader 0:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 165/360 [00:06<00:07, 24.56it/s]Predicting DataLoader 0:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 166/360 [00:06<00:07, 24.55it/s]Predicting DataLoader 0:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 167/360 [00:06<00:07, 24.51it/s]Predicting DataLoader 0:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 168/360 [00:06<00:07, 24.51it/s]Predicting DataLoader 0:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 169/360 [00:06<00:07, 24.49it/s]Predicting DataLoader 0:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 170/360 [00:06<00:07, 24.49it/s]Predicting DataLoader 0:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 171/360 [00:06<00:07, 24.48it/s]Predicting DataLoader 0:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 172/360 [00:07<00:07, 24.47it/s]Predicting DataLoader 0:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 173/360 [00:07<00:07, 24.46it/s]Predicting DataLoader 0:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 174/360 [00:07<00:07, 24.45it/s]Predicting DataLoader 0:  49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 175/360 [00:07<00:07, 24.40it/s]Predicting DataLoader 0:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 176/360 [00:07<00:07, 24.40it/s]Predicting DataLoader 0:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 177/360 [00:07<00:07, 24.38it/s]Predicting DataLoader 0:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 178/360 [00:07<00:07, 24.38it/s]Predicting DataLoader 0:  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 179/360 [00:07<00:07, 24.36it/s]Predicting DataLoader 0:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 180/360 [00:07<00:07, 24.35it/s]Predicting DataLoader 0:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 181/360 [00:07<00:07, 24.35it/s]Predicting DataLoader 0:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 182/360 [00:07<00:07, 24.35it/s]Predicting DataLoader 0:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 183/360 [00:07<00:07, 24.33it/s]Predicting DataLoader 0:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 184/360 [00:07<00:07, 24.31it/s]Predicting DataLoader 0:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 185/360 [00:07<00:07, 24.31it/s]Predicting DataLoader 0:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 186/360 [00:07<00:07, 24.29it/s]Predicting DataLoader 0:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 187/360 [00:07<00:07, 24.28it/s]Predicting DataLoader 0:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 188/360 [00:07<00:07, 24.27it/s]Predicting DataLoader 0:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 189/360 [00:07<00:07, 24.27it/s]Predicting DataLoader 0:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 190/360 [00:07<00:07, 24.25it/s]Predicting DataLoader 0:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 191/360 [00:07<00:06, 24.23it/s]Predicting DataLoader 0:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 192/360 [00:07<00:06, 24.21it/s]Predicting DataLoader 0:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 193/360 [00:07<00:06, 24.20it/s]Predicting DataLoader 0:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 194/360 [00:08<00:06, 24.19it/s]Predicting DataLoader 0:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 195/360 [00:08<00:06, 24.19it/s]Predicting DataLoader 0:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 196/360 [00:08<00:06, 24.19it/s]Predicting DataLoader 0:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 197/360 [00:08<00:06, 24.17it/s]Predicting DataLoader 0:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 198/360 [00:08<00:06, 24.15it/s]Predicting DataLoader 0:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 199/360 [00:08<00:06, 24.14it/s]Predicting DataLoader 0:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 200/360 [00:08<00:06, 24.13it/s]Predicting DataLoader 0:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 201/360 [00:08<00:06, 24.13it/s]Predicting DataLoader 0:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 202/360 [00:08<00:06, 24.12it/s]Predicting DataLoader 0:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 203/360 [00:08<00:06, 24.11it/s]Predicting DataLoader 0:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 204/360 [00:08<00:06, 24.10it/s]Predicting DataLoader 0:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 205/360 [00:08<00:06, 24.09it/s]Predicting DataLoader 0:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 206/360 [00:08<00:06, 24.07it/s]Predicting DataLoader 0:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 207/360 [00:08<00:06, 24.06it/s]Predicting DataLoader 0:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 208/360 [00:08<00:06, 24.05it/s]Predicting DataLoader 0:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 209/360 [00:08<00:06, 24.04it/s]Predicting DataLoader 0:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 210/360 [00:08<00:06, 24.03it/s]Predicting DataLoader 0:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 211/360 [00:08<00:06, 24.01it/s]Predicting DataLoader 0:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 212/360 [00:08<00:06, 24.00it/s]Predicting DataLoader 0:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 213/360 [00:08<00:06, 23.99it/s]Predicting DataLoader 0:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 214/360 [00:08<00:06, 23.97it/s]Predicting DataLoader 0:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 215/360 [00:08<00:06, 23.96it/s]Predicting DataLoader 0:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 216/360 [00:09<00:06, 23.95it/s]Predicting DataLoader 0:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 217/360 [00:09<00:05, 23.93it/s]Predicting DataLoader 0:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 218/360 [00:09<00:05, 23.92it/s]Predicting DataLoader 0:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 219/360 [00:09<00:05, 23.90it/s]Predicting DataLoader 0:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 220/360 [00:09<00:05, 23.89it/s]Predicting DataLoader 0:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 221/360 [00:09<00:05, 23.88it/s]Predicting DataLoader 0:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 222/360 [00:09<00:05, 23.86it/s]Predicting DataLoader 0:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 223/360 [00:09<00:05, 23.83it/s]Predicting DataLoader 0:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 224/360 [00:09<00:05, 23.82it/s]Predicting DataLoader 0:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 225/360 [00:09<00:05, 23.81it/s]Predicting DataLoader 0:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 226/360 [00:09<00:05, 23.79it/s]Predicting DataLoader 0:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 227/360 [00:09<00:05, 23.78it/s]Predicting DataLoader 0:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 228/360 [00:09<00:05, 23.76it/s]Predicting DataLoader 0:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 229/360 [00:09<00:05, 23.75it/s]Predicting DataLoader 0:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 230/360 [00:09<00:05, 23.74it/s]Predicting DataLoader 0:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 231/360 [00:09<00:05, 23.73it/s]Predicting DataLoader 0:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 232/360 [00:09<00:05, 23.72it/s]Predicting DataLoader 0:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 233/360 [00:09<00:05, 23.70it/s]Predicting DataLoader 0:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 234/360 [00:09<00:05, 23.69it/s]Predicting DataLoader 0:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 235/360 [00:09<00:05, 23.67it/s]Predicting DataLoader 0:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 236/360 [00:09<00:05, 23.66it/s]Predicting DataLoader 0:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 237/360 [00:10<00:05, 23.65it/s]Predicting DataLoader 0:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 238/360 [00:10<00:05, 23.64it/s]Predicting DataLoader 0:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 239/360 [00:10<00:05, 23.62it/s]Predicting DataLoader 0:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 240/360 [00:10<00:05, 23.61it/s]Predicting DataLoader 0:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 241/360 [00:10<00:05, 23.58it/s]Predicting DataLoader 0:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 242/360 [00:10<00:05, 23.57it/s]Predicting DataLoader 0:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 243/360 [00:10<00:04, 23.55it/s]Predicting DataLoader 0:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 244/360 [00:10<00:04, 23.54it/s]Predicting DataLoader 0:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 245/360 [00:10<00:04, 23.53it/s]Predicting DataLoader 0:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 246/360 [00:10<00:04, 23.51it/s]Predicting DataLoader 0:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 247/360 [00:10<00:04, 23.50it/s]Predicting DataLoader 0:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 248/360 [00:10<00:04, 23.48it/s]Predicting DataLoader 0:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 249/360 [00:10<00:04, 23.47it/s]Predicting DataLoader 0:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 250/360 [00:10<00:04, 23.45it/s]Predicting DataLoader 0:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 251/360 [00:10<00:04, 23.43it/s]Predicting DataLoader 0:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 252/360 [00:10<00:04, 23.42it/s]Predicting DataLoader 0:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 253/360 [00:10<00:04, 23.40it/s]Predicting DataLoader 0:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 254/360 [00:10<00:04, 23.39it/s]Predicting DataLoader 0:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 255/360 [00:10<00:04, 23.37it/s]Predicting DataLoader 0:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 256/360 [00:10<00:04, 23.36it/s]Predicting DataLoader 0:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 257/360 [00:11<00:04, 23.33it/s]Predicting DataLoader 0:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 258/360 [00:11<00:04, 23.32it/s]Predicting DataLoader 0:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 259/360 [00:11<00:04, 23.31it/s]Predicting DataLoader 0:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 260/360 [00:11<00:04, 23.29it/s]Predicting DataLoader 0:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 261/360 [00:11<00:04, 23.27it/s]Predicting DataLoader 0:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 262/360 [00:11<00:04, 23.25it/s]Predicting DataLoader 0:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 263/360 [00:11<00:04, 23.24it/s]Predicting DataLoader 0:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 264/360 [00:11<00:04, 23.22it/s]Predicting DataLoader 0:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 265/360 [00:11<00:04, 23.20it/s]Predicting DataLoader 0:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 266/360 [00:11<00:04, 23.18it/s]Predicting DataLoader 0:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 267/360 [00:11<00:04, 23.17it/s]Predicting DataLoader 0:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 268/360 [00:11<00:03, 23.15it/s]Predicting DataLoader 0:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 269/360 [00:11<00:03, 23.14it/s]Predicting DataLoader 0:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 270/360 [00:11<00:03, 23.12it/s]Predicting DataLoader 0:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 271/360 [00:11<00:03, 23.11it/s]Predicting DataLoader 0:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 272/360 [00:11<00:03, 23.10it/s]Predicting DataLoader 0:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 273/360 [00:11<00:03, 23.08it/s]Predicting DataLoader 0:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 274/360 [00:11<00:03, 23.07it/s]Predicting DataLoader 0:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 275/360 [00:11<00:03, 23.05it/s]Predicting DataLoader 0:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 276/360 [00:11<00:03, 23.03it/s]Predicting DataLoader 0:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 277/360 [00:12<00:03, 23.02it/s]Predicting DataLoader 0:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 278/360 [00:12<00:03, 23.01it/s]Predicting DataLoader 0:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 279/360 [00:12<00:03, 22.99it/s]Predicting DataLoader 0:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 280/360 [00:12<00:03, 22.98it/s]Predicting DataLoader 0:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 281/360 [00:12<00:03, 22.97it/s]Predicting DataLoader 0:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 282/360 [00:12<00:03, 22.95it/s]Predicting DataLoader 0:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 283/360 [00:12<00:03, 22.94it/s]Predicting DataLoader 0:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 284/360 [00:12<00:03, 22.92it/s]Predicting DataLoader 0:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 285/360 [00:12<00:03, 22.90it/s]Predicting DataLoader 0:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 286/360 [00:12<00:03, 22.89it/s]Predicting DataLoader 0:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 287/360 [00:12<00:03, 22.87it/s]Predicting DataLoader 0:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 288/360 [00:12<00:03, 22.85it/s]Predicting DataLoader 0:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 289/360 [00:12<00:03, 22.83it/s]Predicting DataLoader 0:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 290/360 [00:12<00:03, 22.82it/s]Predicting DataLoader 0:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 291/360 [00:12<00:03, 22.80it/s]Predicting DataLoader 0:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 292/360 [00:12<00:02, 22.78it/s]Predicting DataLoader 0:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 293/360 [00:12<00:02, 22.76it/s]Predicting DataLoader 0:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 294/360 [00:12<00:02, 22.75it/s]Predicting DataLoader 0:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 295/360 [00:12<00:02, 22.73it/s]Predicting DataLoader 0:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 296/360 [00:13<00:02, 22.72it/s]Predicting DataLoader 0:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 297/360 [00:13<00:02, 22.70it/s]Predicting DataLoader 0:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 298/360 [00:13<00:02, 22.68it/s]Predicting DataLoader 0:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 299/360 [00:13<00:02, 22.66it/s]Predicting DataLoader 0:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 300/360 [00:13<00:02, 22.65it/s]Predicting DataLoader 0:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 301/360 [00:13<00:02, 22.63it/s]Predicting DataLoader 0:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 302/360 [00:13<00:02, 22.62it/s]Predicting DataLoader 0:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 303/360 [00:13<00:02, 22.60it/s]Predicting DataLoader 0:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 304/360 [00:13<00:02, 22.58it/s]Predicting DataLoader 0:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 305/360 [00:13<00:02, 22.57it/s]Predicting DataLoader 0:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 306/360 [00:13<00:02, 22.54it/s]Predicting DataLoader 0:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 307/360 [00:13<00:02, 22.52it/s]Predicting DataLoader 0:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 308/360 [00:13<00:02, 22.51it/s]Predicting DataLoader 0:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 309/360 [00:13<00:02, 22.49it/s]Predicting DataLoader 0:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 310/360 [00:13<00:02, 22.48it/s]Predicting DataLoader 0:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 311/360 [00:13<00:02, 22.45it/s]Predicting DataLoader 0:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 312/360 [00:13<00:02, 22.43it/s]Predicting DataLoader 0:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 313/360 [00:13<00:02, 22.41it/s]Predicting DataLoader 0:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 314/360 [00:14<00:02, 22.40it/s]Predicting DataLoader 0:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 315/360 [00:14<00:02, 22.38it/s]Predicting DataLoader 0:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 316/360 [00:14<00:01, 22.37it/s]Predicting DataLoader 0:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 317/360 [00:14<00:01, 22.35it/s]Predicting DataLoader 0:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 318/360 [00:14<00:01, 22.33it/s]Predicting DataLoader 0:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 319/360 [00:14<00:01, 22.31it/s]Predicting DataLoader 0:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 320/360 [00:14<00:01, 22.30it/s]Predicting DataLoader 0:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 321/360 [00:14<00:01, 22.28it/s]Predicting DataLoader 0:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 322/360 [00:14<00:01, 22.25it/s]Predicting DataLoader 0:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 323/360 [00:14<00:01, 22.24it/s]Predicting DataLoader 0:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 324/360 [00:14<00:01, 22.22it/s]Predicting DataLoader 0:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 325/360 [00:14<00:01, 22.20it/s]Predicting DataLoader 0:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 326/360 [00:14<00:01, 22.18it/s]Predicting DataLoader 0:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 327/360 [00:14<00:01, 22.16it/s]Predicting DataLoader 0:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 328/360 [00:14<00:01, 22.15it/s]Predicting DataLoader 0:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 329/360 [00:14<00:01, 22.13it/s]Predicting DataLoader 0:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 330/360 [00:14<00:01, 22.11it/s]Predicting DataLoader 0:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 331/360 [00:15<00:01, 22.07it/s]Predicting DataLoader 0:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 332/360 [00:15<00:01, 22.04it/s]Predicting DataLoader 0:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 333/360 [00:15<00:01, 22.02it/s]Predicting DataLoader 0:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 334/360 [00:15<00:01, 22.00it/s]Predicting DataLoader 0:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 335/360 [00:15<00:01, 21.98it/s]Predicting DataLoader 0:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 336/360 [00:15<00:01, 21.96it/s]Predicting DataLoader 0:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 337/360 [00:15<00:01, 21.94it/s]Predicting DataLoader 0:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 338/360 [00:15<00:01, 21.92it/s]Predicting DataLoader 0:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 339/360 [00:15<00:00, 21.90it/s]Predicting DataLoader 0:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 340/360 [00:15<00:00, 21.88it/s]Predicting DataLoader 0:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 341/360 [00:15<00:00, 21.86it/s]Predicting DataLoader 0:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 342/360 [00:15<00:00, 21.84it/s]Predicting DataLoader 0:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 343/360 [00:15<00:00, 21.81it/s]Predicting DataLoader 0:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 344/360 [00:15<00:00, 21.78it/s]Predicting DataLoader 0:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 345/360 [00:15<00:00, 21.76it/s]Predicting DataLoader 0:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 346/360 [00:15<00:00, 21.73it/s]Predicting DataLoader 0:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 347/360 [00:15<00:00, 21.70it/s]Predicting DataLoader 0:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 348/360 [00:16<00:00, 21.67it/s]Predicting DataLoader 0:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 349/360 [00:16<00:00, 21.65it/s]Predicting DataLoader 0:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 350/360 [00:16<00:00, 21.60it/s]Predicting DataLoader 0:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 351/360 [00:16<00:00, 21.57it/s]Predicting DataLoader 0:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 352/360 [00:16<00:00, 21.55it/s]Predicting DataLoader 0:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 353/360 [00:16<00:00, 21.51it/s]Predicting DataLoader 0:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 354/360 [00:16<00:00, 21.47it/s]Predicting DataLoader 0:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 355/360 [00:16<00:00, 21.43it/s]Predicting DataLoader 0:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 356/360 [00:16<00:00, 21.37it/s]Predicting DataLoader 0:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 357/360 [00:16<00:00, 21.26it/s]Predicting DataLoader 0:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 358/360 [00:16<00:00, 21.14it/s]Predicting DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 359/360 [00:17<00:00, 20.91it/s]Predicting DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 360/360 [00:17<00:00, 20.87it/s]Predicting DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 360/360 [00:17<00:00, 20.87it/s]
Traceback (most recent call last):
  File "/home/ivieira/chicago2/HP_FT_TM/inference_eval.py", line 120, in <module>
    bleu = sacrebleu.corpus_bleu(predictions, [references])
                ^^^^^^^^^^^^^^^^^
TypeError: unsupported operand type(s) for +: 'int' and 'str'
