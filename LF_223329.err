[INFO|configuration_utils.py:679] 2025-01-15 14:24:38,339 >> loading configuration file config.json from cache at /home/ivieira/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659/config.json
[INFO|configuration_utils.py:746] 2025-01-15 14:24:38,340 >> Model config LlamaConfig {
  "_name_or_path": "meta-llama/Meta-Llama-3.1-8B-Instruct",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.1",
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|tokenization_utils_base.py:2211] 2025-01-15 14:24:38,531 >> loading file tokenizer.json from cache at /home/ivieira/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659/tokenizer.json
[INFO|tokenization_utils_base.py:2211] 2025-01-15 14:24:38,532 >> loading file tokenizer.model from cache at None
[INFO|tokenization_utils_base.py:2211] 2025-01-15 14:24:38,532 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2211] 2025-01-15 14:24:38,532 >> loading file special_tokens_map.json from cache at /home/ivieira/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659/special_tokens_map.json
[INFO|tokenization_utils_base.py:2211] 2025-01-15 14:24:38,532 >> loading file tokenizer_config.json from cache at /home/ivieira/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659/tokenizer_config.json
[INFO|tokenization_utils_base.py:2475] 2025-01-15 14:24:39,024 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:679] 2025-01-15 14:24:39,792 >> loading configuration file config.json from cache at /home/ivieira/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659/config.json
[INFO|configuration_utils.py:746] 2025-01-15 14:24:39,801 >> Model config LlamaConfig {
  "_name_or_path": "meta-llama/Meta-Llama-3.1-8B-Instruct",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.1",
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|tokenization_utils_base.py:2211] 2025-01-15 14:24:40,050 >> loading file tokenizer.json from cache at /home/ivieira/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659/tokenizer.json
[INFO|tokenization_utils_base.py:2211] 2025-01-15 14:24:40,066 >> loading file tokenizer.model from cache at None
[INFO|tokenization_utils_base.py:2211] 2025-01-15 14:24:40,074 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2211] 2025-01-15 14:24:40,083 >> loading file special_tokens_map.json from cache at /home/ivieira/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659/special_tokens_map.json
[INFO|tokenization_utils_base.py:2211] 2025-01-15 14:24:40,090 >> loading file tokenizer_config.json from cache at /home/ivieira/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659/tokenizer_config.json
[INFO|tokenization_utils_base.py:2475] 2025-01-15 14:24:40,584 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Converting format of dataset (num_proc=16):   0%|          | 0/25856 [00:00<?, ? examples/s]Converting format of dataset (num_proc=16):   2%|▏         | 437/25856 [00:00<00:07, 3231.01 examples/s]Converting format of dataset (num_proc=16):  10%|▉         | 2473/25856 [00:00<00:02, 11508.85 examples/s]Converting format of dataset (num_proc=16):  20%|██        | 5204/25856 [00:00<00:01, 16738.63 examples/s]Converting format of dataset (num_proc=16):  38%|███▊      | 9858/25856 [00:00<00:00, 27263.46 examples/s]Converting format of dataset (num_proc=16):  52%|█████▏    | 13416/25856 [00:00<00:00, 29814.33 examples/s]Converting format of dataset (num_proc=16):  64%|██████▍   | 16602/25856 [00:00<00:00, 29592.74 examples/s]Converting format of dataset (num_proc=16):  76%|███████▋  | 19773/25856 [00:00<00:00, 27606.48 examples/s]Converting format of dataset (num_proc=16):  89%|████████▉ | 22982/25856 [00:00<00:00, 28786.97 examples/s]Converting format of dataset (num_proc=16): 100%|██████████| 25856/25856 [00:01<00:00, 24058.51 examples/s]
Running tokenizer on dataset (num_proc=16):   0%|          | 0/25856 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=16):   4%|▍         | 1000/25856 [00:02<01:01, 404.56 examples/s]Running tokenizer on dataset (num_proc=16):   8%|▊         | 2000/25856 [00:03<00:33, 709.81 examples/s]Running tokenizer on dataset (num_proc=16):  10%|█         | 2616/25856 [00:03<00:30, 763.77 examples/s]Running tokenizer on dataset (num_proc=16):  23%|██▎       | 5848/25856 [00:04<00:10, 1950.72 examples/s]Running tokenizer on dataset (num_proc=16):  26%|██▋       | 6848/25856 [00:05<00:10, 1816.30 examples/s]Running tokenizer on dataset (num_proc=16):  33%|███▎      | 8464/25856 [00:05<00:08, 1999.17 examples/s]Running tokenizer on dataset (num_proc=16):  41%|████▏     | 10696/25856 [00:06<00:06, 2515.48 examples/s]Running tokenizer on dataset (num_proc=16):  45%|████▌     | 11696/25856 [00:06<00:06, 2299.84 examples/s]Running tokenizer on dataset (num_proc=16):  55%|█████▌    | 14312/25856 [00:07<00:04, 2776.48 examples/s]Running tokenizer on dataset (num_proc=16):  62%|██████▎   | 16160/25856 [00:08<00:03, 2949.79 examples/s]Running tokenizer on dataset (num_proc=16):  74%|███████▍  | 19160/25856 [00:08<00:02, 3260.93 examples/s]Running tokenizer on dataset (num_proc=16):  83%|████████▎ | 21392/25856 [00:09<00:01, 4269.46 examples/s]Running tokenizer on dataset (num_proc=16):  89%|████████▉ | 23008/25856 [00:09<00:00, 4613.63 examples/s]Running tokenizer on dataset (num_proc=16):  94%|█████████▍| 24240/25856 [00:09<00:00, 4242.77 examples/s]Running tokenizer on dataset (num_proc=16):  98%|█████████▊| 25240/25856 [00:09<00:00, 4758.29 examples/s]Running tokenizer on dataset (num_proc=16): 100%|██████████| 25856/25856 [00:10<00:00, 2547.03 examples/s]
[INFO|configuration_utils.py:679] 2025-01-15 14:24:53,216 >> loading configuration file config.json from cache at /home/ivieira/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659/config.json
[INFO|configuration_utils.py:746] 2025-01-15 14:24:53,230 >> Model config LlamaConfig {
  "_name_or_path": "meta-llama/Meta-Llama-3.1-8B-Instruct",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.1",
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|modeling_utils.py:3937] 2025-01-15 14:24:53,690 >> loading weights file model.safetensors from cache at /home/ivieira/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659/model.safetensors.index.json
[INFO|modeling_utils.py:1670] 2025-01-15 14:24:53,710 >> Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.
[INFO|configuration_utils.py:1096] 2025-01-15 14:24:53,720 >> Generate config GenerationConfig {
  "bos_token_id": 128000,
  "eos_token_id": [
    128001,
    128008,
    128009
  ]
}

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:05<00:15,  5.24s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:10<00:10,  5.18s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:15<00:05,  5.10s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:16<00:00,  3.57s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:16<00:00,  4.16s/it]
[INFO|modeling_utils.py:4800] 2025-01-15 14:25:10,552 >> All model checkpoint weights were used when initializing LlamaForCausalLM.

[INFO|modeling_utils.py:4808] 2025-01-15 14:25:10,560 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at meta-llama/Meta-Llama-3.1-8B-Instruct.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
[INFO|configuration_utils.py:1051] 2025-01-15 14:25:10,771 >> loading configuration file generation_config.json from cache at /home/ivieira/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659/generation_config.json
[INFO|configuration_utils.py:1096] 2025-01-15 14:25:10,788 >> Generate config GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "temperature": 0.6,
  "top_p": 0.9
}

[INFO|trainer.py:698] 2025-01-15 14:25:12,842 >> Using auto half precision backend
[INFO|trainer.py:2313] 2025-01-15 14:25:13,084 >> ***** Running training *****
[INFO|trainer.py:2314] 2025-01-15 14:25:13,099 >>   Num examples = 23,270
[INFO|trainer.py:2315] 2025-01-15 14:25:13,107 >>   Num Epochs = 1
[INFO|trainer.py:2316] 2025-01-15 14:25:13,115 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:2319] 2025-01-15 14:25:13,123 >>   Total train batch size (w. parallel, distributed & accumulation) = 256
[INFO|trainer.py:2320] 2025-01-15 14:25:13,131 >>   Gradient Accumulation steps = 8
[INFO|trainer.py:2321] 2025-01-15 14:25:13,139 >>   Total optimization steps = 1
[INFO|trainer.py:2322] 2025-01-15 14:25:13,158 >>   Number of trainable parameters = 167,772,160
[INFO|integration_utils.py:812] 2025-01-15 14:25:13,174 >> Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
wandb: Currently logged in as: inaciovieira (inaciovieira-alpha-crc). Use `wandb login --relogin` to force relogin
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.2
wandb: Run data is saved locally in /home/ivieira/LLaMA-Factory/wandb/run-20250115_142513-r5zw755q
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run train_2025-01-15-14-24-31
wandb: ⭐️ View project at https://wandb.ai/inaciovieira-alpha-crc/llamafactory
wandb: 🚀 View run at https://wandb.ai/inaciovieira-alpha-crc/llamafactory/runs/r5zw755q
  0%|          | 0/1 [00:00<?, ?it/s]/home/ivieira/miniconda3/envs/lf-env/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/home/ivieira/miniconda3/envs/lf-env/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
100%|██████████| 1/1 [00:55<00:00, 55.26s/it][INFO|trainer.py:3801] 2025-01-15 14:26:09,766 >> Saving model checkpoint to saves/Llama-3.1-8B-Instruct/lora/train_2025-01-15-14-24-31/checkpoint-1
[INFO|configuration_utils.py:679] 2025-01-15 14:26:10,200 >> loading configuration file config.json from cache at /home/ivieira/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659/config.json
[INFO|configuration_utils.py:746] 2025-01-15 14:26:10,220 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.1",
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|tokenization_utils_base.py:2646] 2025-01-15 14:26:10,983 >> tokenizer config file saved in saves/Llama-3.1-8B-Instruct/lora/train_2025-01-15-14-24-31/checkpoint-1/tokenizer_config.json
[INFO|tokenization_utils_base.py:2655] 2025-01-15 14:26:10,998 >> Special tokens file saved in saves/Llama-3.1-8B-Instruct/lora/train_2025-01-15-14-24-31/checkpoint-1/special_tokens_map.json
[INFO|trainer.py:2584] 2025-01-15 14:26:12,398 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                             100%|██████████| 1/1 [00:57<00:00, 55.26s/it]100%|██████████| 1/1 [00:57<00:00, 57.95s/it]
[INFO|trainer.py:3801] 2025-01-15 14:26:12,461 >> Saving model checkpoint to saves/Llama-3.1-8B-Instruct/lora/train_2025-01-15-14-24-31
[INFO|configuration_utils.py:679] 2025-01-15 14:26:13,007 >> loading configuration file config.json from cache at /home/ivieira/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659/config.json
[INFO|configuration_utils.py:746] 2025-01-15 14:26:13,028 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.1",
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|tokenization_utils_base.py:2646] 2025-01-15 14:26:13,798 >> tokenizer config file saved in saves/Llama-3.1-8B-Instruct/lora/train_2025-01-15-14-24-31/tokenizer_config.json
[INFO|tokenization_utils_base.py:2655] 2025-01-15 14:26:13,811 >> Special tokens file saved in saves/Llama-3.1-8B-Instruct/lora/train_2025-01-15-14-24-31/special_tokens_map.json
[INFO|trainer.py:4117] 2025-01-15 14:26:14,004 >> 
***** Running Evaluation *****
[INFO|trainer.py:4119] 2025-01-15 14:26:14,015 >>   Num examples = 2586
[INFO|trainer.py:4122] 2025-01-15 14:26:14,024 >>   Batch size = 16
/home/ivieira/miniconda3/envs/lf-env/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/home/ivieira/miniconda3/envs/lf-env/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
  0%|          | 0/162 [00:00<?, ?it/s]  1%|          | 2/162 [00:00<00:41,  3.89it/s]  2%|▏         | 3/162 [00:00<00:55,  2.85it/s]  2%|▏         | 4/162 [00:01<01:05,  2.42it/s]  3%|▎         | 5/162 [00:02<01:18,  2.00it/s]  4%|▎         | 6/162 [00:02<01:17,  2.02it/s]  4%|▍         | 7/162 [00:03<01:16,  2.02it/s]  5%|▍         | 8/162 [00:03<01:24,  1.83it/s]  6%|▌         | 9/162 [00:04<01:21,  1.88it/s]  6%|▌         | 10/162 [00:05<01:28,  1.72it/s]  7%|▋         | 11/162 [00:05<01:41,  1.49it/s]  7%|▋         | 12/162 [00:06<01:41,  1.48it/s]  8%|▊         | 13/162 [00:07<02:00,  1.24it/s]  9%|▊         | 14/162 [00:08<01:46,  1.38it/s]  9%|▉         | 15/162 [00:08<01:35,  1.53it/s] 10%|▉         | 16/162 [00:09<01:27,  1.66it/s] 10%|█         | 17/162 [00:09<01:25,  1.69it/s] 11%|█         | 18/162 [00:10<01:28,  1.63it/s] 12%|█▏        | 19/162 [00:10<01:24,  1.69it/s] 12%|█▏        | 20/162 [00:11<01:28,  1.60it/s] 13%|█▎        | 21/162 [00:12<01:23,  1.68it/s] 14%|█▎        | 22/162 [00:12<01:18,  1.79it/s] 14%|█▍        | 23/162 [00:13<01:27,  1.58it/s]slurmstepd-g128: error: *** JOB 223329 ON g128 CANCELLED AT 2025-01-15T14:26:28 ***
