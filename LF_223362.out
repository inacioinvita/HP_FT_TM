CUDA available? True
Device count: 1
[WARNING|2025-01-15 17:01:09] llamafactory.hparams.parser:162 >> We recommend enable `upcast_layernorm` in quantized training.
[INFO|2025-01-15 17:01:09] llamafactory.hparams.parser:373 >> Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: torch.bfloat16
[INFO|2025-01-15 17:01:12] llamafactory.data.template:157 >> Add pad token: <|eot_id|>
[INFO|2025-01-15 17:01:12] llamafactory.data.template:157 >> Add <|eot_id|>,<|eom_id|> to stop words.
[INFO|2025-01-15 17:01:12] llamafactory.data.loader:157 >> Loading dataset BALS_de_train_dataset.json...
training example:
input_ids:
[128000, 128006, 9125, 128007, 271, 2675, 527, 264, 28175, 14807, 15592, 13, 1472, 690, 5371, 6498, 1495, 323, 28832, 6013, 27785, 304, 4823, 13, 578, 4823, 2011, 617, 279, 1401, 364, 3129, 4527, 13688, 3645, 25, 5324, 3129, 3332, 82116, 1495, 1, 7966, 3234, 539, 923, 5066, 31710, 13, 128009, 128006, 882, 128007, 271, 5618, 15025, 279, 2768, 1495, 505, 6498, 311, 6063, 627, 37, 7618, 18977, 50, 128009, 128006, 78191, 128007, 271, 11673, 52, 643, 40451, 42, 320, 37, 7618, 18977, 50, 8, 128009]
inputs:
<|begin_of_text|><|start_header_id|>system<|end_header_id|>

You are a specialized translation AI. You will receive English text and MUST respond ONLY in JSON. The JSON must have the key 'translation'. Example format: {"translation":"Translated text"}. Do not add extra commentary.<|eot_id|><|start_header_id|>user<|end_header_id|>

Please translate the following text from English to German.
FIREWORKS<|eot_id|><|start_header_id|>assistant<|end_header_id|>

FEUERWERK (FIREWORKS)<|eot_id|>
label_ids:
[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 11673, 52, 643, 40451, 42, 320, 37, 7618, 18977, 50, 8, 128009]
labels:
FEUERWERK (FIREWORKS)<|eot_id|>
[INFO|2025-01-15 17:01:13] llamafactory.model.model_utils.quantization:157 >> Quantizing model to 4 bit with bitsandbytes.
[INFO|2025-01-15 17:01:21] llamafactory.model.model_utils.checkpointing:157 >> Gradient checkpointing enabled.
[INFO|2025-01-15 17:01:21] llamafactory.model.model_utils.attention:157 >> Using torch SDPA for faster training and inference.
[INFO|2025-01-15 17:01:21] llamafactory.model.adapter:157 >> Upcasting trainable params to float32.
[INFO|2025-01-15 17:01:21] llamafactory.model.adapter:157 >> Fine-tuning method: LoRA
[INFO|2025-01-15 17:01:21] llamafactory.model.model_utils.misc:157 >> Found linear modules: q_proj,gate_proj,down_proj,up_proj,v_proj,k_proj,o_proj
[INFO|2025-01-15 17:01:23] llamafactory.model.loader:157 >> trainable params: 167,772,160 || all params: 8,198,033,408 || trainable%: 2.0465
{'train_runtime': 19.5628, 'train_samples_per_second': 11.895, 'train_steps_per_second': 0.051, 'train_loss': 2.305986166000366, 'epoch': 0.01}
***** train metrics *****
  epoch                    =      0.011
  total_flos               =  2030537GF
  train_loss               =      2.306
  train_runtime            = 0:00:19.56
  train_samples_per_second =     11.895
  train_steps_per_second   =      0.051
[WARNING|2025-01-15 17:01:45] llamafactory.extras.ploting:162 >> No metric loss to plot.
[WARNING|2025-01-15 17:01:45] llamafactory.extras.ploting:162 >> No metric eval_loss to plot.
[WARNING|2025-01-15 17:01:45] llamafactory.extras.ploting:162 >> No metric eval_accuracy to plot.
