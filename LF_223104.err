[INFO|configuration_utils.py:679] 2025-01-13 16:50:46,912 >> loading configuration file config.json from cache at /home/ivieira/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659/config.json
[INFO|configuration_utils.py:746] 2025-01-13 16:50:46,928 >> Model config LlamaConfig {
  "_name_or_path": "meta-llama/Meta-Llama-3.1-8B-Instruct",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.1",
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|tokenization_utils_base.py:2211] 2025-01-13 16:50:47,201 >> loading file tokenizer.json from cache at /home/ivieira/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659/tokenizer.json
[INFO|tokenization_utils_base.py:2211] 2025-01-13 16:50:47,224 >> loading file tokenizer.model from cache at None
[INFO|tokenization_utils_base.py:2211] 2025-01-13 16:50:47,235 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2211] 2025-01-13 16:50:47,245 >> loading file special_tokens_map.json from cache at /home/ivieira/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659/special_tokens_map.json
[INFO|tokenization_utils_base.py:2211] 2025-01-13 16:50:47,254 >> loading file tokenizer_config.json from cache at /home/ivieira/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659/tokenizer_config.json
[INFO|tokenization_utils_base.py:2475] 2025-01-13 16:50:47,705 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:679] 2025-01-13 16:50:48,977 >> loading configuration file config.json from cache at /home/ivieira/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659/config.json
[INFO|configuration_utils.py:746] 2025-01-13 16:50:48,991 >> Model config LlamaConfig {
  "_name_or_path": "meta-llama/Meta-Llama-3.1-8B-Instruct",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.1",
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|tokenization_utils_base.py:2211] 2025-01-13 16:50:49,208 >> loading file tokenizer.json from cache at /home/ivieira/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659/tokenizer.json
[INFO|tokenization_utils_base.py:2211] 2025-01-13 16:50:49,232 >> loading file tokenizer.model from cache at None
[INFO|tokenization_utils_base.py:2211] 2025-01-13 16:50:49,242 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2211] 2025-01-13 16:50:49,251 >> loading file special_tokens_map.json from cache at /home/ivieira/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659/special_tokens_map.json
[INFO|tokenization_utils_base.py:2211] 2025-01-13 16:50:49,260 >> loading file tokenizer_config.json from cache at /home/ivieira/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659/tokenizer_config.json
[INFO|tokenization_utils_base.py:2475] 2025-01-13 16:50:49,703 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:679] 2025-01-13 16:50:50,618 >> loading configuration file config.json from cache at /home/ivieira/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659/config.json
[INFO|configuration_utils.py:746] 2025-01-13 16:50:50,630 >> Model config LlamaConfig {
  "_name_or_path": "meta-llama/Meta-Llama-3.1-8B-Instruct",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.1",
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|modeling_utils.py:3937] 2025-01-13 16:50:50,770 >> loading weights file model.safetensors from cache at /home/ivieira/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659/model.safetensors.index.json
[INFO|modeling_utils.py:1670] 2025-01-13 16:50:50,781 >> Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.
[INFO|configuration_utils.py:1096] 2025-01-13 16:50:50,793 >> Generate config GenerationConfig {
  "bos_token_id": 128000,
  "eos_token_id": [
    128001,
    128008,
    128009
  ]
}

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|‚ñà‚ñà‚ñå       | 1/4 [00:02<00:08,  2.96s/it]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 2/4 [00:05<00:05,  2.92s/it]Loading checkpoint shards:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 3/4 [00:08<00:02,  2.85s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:09<00:00,  1.97s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:09<00:00,  2.32s/it]
[INFO|modeling_utils.py:4800] 2025-01-13 16:51:00,240 >> All model checkpoint weights were used when initializing LlamaForCausalLM.

[INFO|modeling_utils.py:4808] 2025-01-13 16:51:00,250 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at meta-llama/Meta-Llama-3.1-8B-Instruct.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
[INFO|configuration_utils.py:1051] 2025-01-13 16:51:00,483 >> loading configuration file generation_config.json from cache at /home/ivieira/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659/generation_config.json
[INFO|configuration_utils.py:1096] 2025-01-13 16:51:00,495 >> Generate config GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "temperature": 0.6,
  "top_p": 0.9
}

[INFO|trainer.py:698] 2025-01-13 16:51:02,457 >> Using auto half precision backend
[INFO|trainer.py:2313] 2025-01-13 16:51:02,706 >> ***** Running training *****
[INFO|trainer.py:2314] 2025-01-13 16:51:02,733 >>   Num examples = 23,270
[INFO|trainer.py:2315] 2025-01-13 16:51:02,744 >>   Num Epochs = 1
[INFO|trainer.py:2316] 2025-01-13 16:51:02,753 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:2319] 2025-01-13 16:51:02,763 >>   Total train batch size (w. parallel, distributed & accumulation) = 512
[INFO|trainer.py:2320] 2025-01-13 16:51:02,772 >>   Gradient Accumulation steps = 16
[INFO|trainer.py:2321] 2025-01-13 16:51:02,782 >>   Total optimization steps = 45
[INFO|trainer.py:2322] 2025-01-13 16:51:02,800 >>   Number of trainable parameters = 167,772,160
[INFO|integration_utils.py:812] 2025-01-13 16:51:02,820 >> Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
wandb: Currently logged in as: inaciovieira (inaciovieira-alpha-crc). Use `wandb login --relogin` to force relogin
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.2
wandb: Run data is saved locally in /home/ivieira/LLaMA-Factory/wandb/run-20250113_165103-kmqfs9p1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run train_2025-01-13-16-50-39
wandb: ‚≠êÔ∏è View project at https://wandb.ai/inaciovieira-alpha-crc/llamafactory
wandb: üöÄ View run at https://wandb.ai/inaciovieira-alpha-crc/llamafactory/runs/kmqfs9p1
  0%|          | 0/45 [00:00<?, ?it/s]/home/ivieira/miniconda3/envs/lf-env/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/home/ivieira/miniconda3/envs/lf-env/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
  2%|‚ñè         | 1/45 [01:48<1:19:40, 108.64s/it]  4%|‚ñç         | 2/45 [03:59<1:27:10, 121.65s/it]  7%|‚ñã         | 3/45 [05:57<1:24:09, 120.22s/it]  9%|‚ñâ         | 4/45 [08:12<1:26:00, 125.86s/it] 11%|‚ñà         | 5/45 [10:03<1:20:16, 120.41s/it] 13%|‚ñà‚ñé        | 6/45 [12:27<1:23:35, 128.59s/it] 16%|‚ñà‚ñå        | 7/45 [14:27<1:19:36, 125.70s/it] 18%|‚ñà‚ñä        | 8/45 [16:59<1:22:36, 133.97s/it] 20%|‚ñà‚ñà        | 9/45 [18:55<1:17:08, 128.57s/it] 22%|‚ñà‚ñà‚ñè       | 10/45 [21:02<1:14:44, 128.13s/it]                                                   22%|‚ñà‚ñà‚ñè       | 10/45 [21:02<1:14:44, 128.13s/it][INFO|trainer.py:3801] 2025-01-13 17:12:07,155 >> Saving model checkpoint to saves/Llama-3.1-8B-Instruct/lora/train_2025-01-13-16-50-39/checkpoint-10
[INFO|configuration_utils.py:679] 2025-01-13 17:12:07,669 >> loading configuration file config.json from cache at /home/ivieira/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659/config.json
[INFO|configuration_utils.py:746] 2025-01-13 17:12:07,671 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.1",
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|tokenization_utils_base.py:2646] 2025-01-13 17:12:08,432 >> tokenizer config file saved in saves/Llama-3.1-8B-Instruct/lora/train_2025-01-13-16-50-39/checkpoint-10/tokenizer_config.json
[INFO|tokenization_utils_base.py:2655] 2025-01-13 17:12:08,434 >> Special tokens file saved in saves/Llama-3.1-8B-Instruct/lora/train_2025-01-13-16-50-39/checkpoint-10/special_tokens_map.json
/home/ivieira/miniconda3/envs/lf-env/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/home/ivieira/miniconda3/envs/lf-env/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
 24%|‚ñà‚ñà‚ñç       | 11/45 [23:02<1:11:11, 125.65s/it] 27%|‚ñà‚ñà‚ñã       | 12/45 [25:05<1:08:36, 124.75s/it] 29%|‚ñà‚ñà‚ñâ       | 13/45 [27:08<1:06:11, 124.11s/it] 31%|‚ñà‚ñà‚ñà       | 14/45 [29:28<1:06:34, 128.85s/it] 33%|‚ñà‚ñà‚ñà‚ñé      | 15/45 [31:35<1:04:10, 128.35s/it] 36%|‚ñà‚ñà‚ñà‚ñå      | 16/45 [33:58<1:04:14, 132.93s/it] 38%|‚ñà‚ñà‚ñà‚ñä      | 17/45 [36:28<1:04:25, 138.07s/it] 40%|‚ñà‚ñà‚ñà‚ñà      | 18/45 [39:00<1:04:00, 142.23s/it] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 19/45 [41:15<1:00:39, 139.99s/it] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 20/45 [43:09<55:06, 132.24s/it]                                                   44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 20/45 [43:09<55:06, 132.24s/it][INFO|trainer.py:3801] 2025-01-13 17:34:13,961 >> Saving model checkpoint to saves/Llama-3.1-8B-Instruct/lora/train_2025-01-13-16-50-39/checkpoint-20
[INFO|configuration_utils.py:679] 2025-01-13 17:34:14,420 >> loading configuration file config.json from cache at /home/ivieira/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659/config.json
[INFO|configuration_utils.py:746] 2025-01-13 17:34:14,422 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.1",
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|tokenization_utils_base.py:2646] 2025-01-13 17:34:15,113 >> tokenizer config file saved in saves/Llama-3.1-8B-Instruct/lora/train_2025-01-13-16-50-39/checkpoint-20/tokenizer_config.json
[INFO|tokenization_utils_base.py:2655] 2025-01-13 17:34:15,114 >> Special tokens file saved in saves/Llama-3.1-8B-Instruct/lora/train_2025-01-13-16-50-39/checkpoint-20/special_tokens_map.json
/home/ivieira/miniconda3/envs/lf-env/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/home/ivieira/miniconda3/envs/lf-env/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 21/45 [45:03<50:37, 126.57s/it] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 22/45 [47:00<47:31, 123.96s/it] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 23/45 [49:14<46:32, 126.93s/it] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 24/45 [51:41<46:29, 132.82s/it] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 25/45 [54:15<46:24, 139.24s/it] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 26/45 [56:24<43:03, 136.00s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 27/45 [58:50<41:42, 139.04s/it] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 28/45 [1:00:40<36:55, 130.32s/it] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 29/45 [1:03:14<36:41, 137.61s/it] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 30/45 [1:05:38<34:52, 139.53s/it]                                                   67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 30/45 [1:05:38<34:52, 139.53s/it][INFO|trainer.py:3801] 2025-01-13 17:56:42,994 >> Saving model checkpoint to saves/Llama-3.1-8B-Instruct/lora/train_2025-01-13-16-50-39/checkpoint-30
[INFO|configuration_utils.py:679] 2025-01-13 17:56:44,061 >> loading configuration file config.json from cache at /home/ivieira/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659/config.json
[INFO|configuration_utils.py:746] 2025-01-13 17:56:44,064 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.1",
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|tokenization_utils_base.py:2646] 2025-01-13 17:56:44,840 >> tokenizer config file saved in saves/Llama-3.1-8B-Instruct/lora/train_2025-01-13-16-50-39/checkpoint-30/tokenizer_config.json
[INFO|tokenization_utils_base.py:2655] 2025-01-13 17:56:44,841 >> Special tokens file saved in saves/Llama-3.1-8B-Instruct/lora/train_2025-01-13-16-50-39/checkpoint-30/special_tokens_map.json
/home/ivieira/miniconda3/envs/lf-env/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/home/ivieira/miniconda3/envs/lf-env/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 31/45 [1:07:49<31:55, 136.80s/it] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 32/45 [1:09:55<28:55, 133.53s/it] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 33/45 [1:12:01<26:17, 131.43s/it] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 34/45 [1:13:59<23:20, 127.28s/it] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 35/45 [1:15:50<20:24, 122.48s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 36/45 [1:17:22<16:58, 113.22s/it] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 37/45 [1:19:06<14:45, 110.64s/it] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 38/45 [1:21:00<13:00, 111.55s/it] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 39/45 [1:22:43<10:54, 109.10s/it] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 40/45 [1:24:34<09:07, 109.56s/it]                                                   89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 40/45 [1:24:34<09:07, 109.56s/it][INFO|trainer.py:3801] 2025-01-13 18:15:38,638 >> Saving model checkpoint to saves/Llama-3.1-8B-Instruct/lora/train_2025-01-13-16-50-39/checkpoint-40
[INFO|configuration_utils.py:679] 2025-01-13 18:15:39,098 >> loading configuration file config.json from cache at /home/ivieira/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659/config.json
[INFO|configuration_utils.py:746] 2025-01-13 18:15:39,100 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.1",
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|tokenization_utils_base.py:2646] 2025-01-13 18:15:39,744 >> tokenizer config file saved in saves/Llama-3.1-8B-Instruct/lora/train_2025-01-13-16-50-39/checkpoint-40/tokenizer_config.json
[INFO|tokenization_utils_base.py:2655] 2025-01-13 18:15:39,745 >> Special tokens file saved in saves/Llama-3.1-8B-Instruct/lora/train_2025-01-13-16-50-39/checkpoint-40/special_tokens_map.json
/home/ivieira/miniconda3/envs/lf-env/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/home/ivieira/miniconda3/envs/lf-env/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 41/45 [1:26:35<07:32, 113.17s/it] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 42/45 [1:28:47<05:56, 118.68s/it] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 43/45 [1:31:12<04:12, 126.49s/it] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 44/45 [1:33:23<02:07, 127.97s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 45/45 [1:36:08<00:00, 139.01s/it][INFO|trainer.py:3801] 2025-01-13 18:27:12,687 >> Saving model checkpoint to saves/Llama-3.1-8B-Instruct/lora/train_2025-01-13-16-50-39/checkpoint-45
[INFO|configuration_utils.py:679] 2025-01-13 18:27:23,426 >> loading configuration file config.json from cache at /home/ivieira/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659/config.json
[INFO|configuration_utils.py:746] 2025-01-13 18:27:23,429 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.1",
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|tokenization_utils_base.py:2646] 2025-01-13 18:27:24,135 >> tokenizer config file saved in saves/Llama-3.1-8B-Instruct/lora/train_2025-01-13-16-50-39/checkpoint-45/tokenizer_config.json
[INFO|tokenization_utils_base.py:2655] 2025-01-13 18:27:24,136 >> Special tokens file saved in saves/Llama-3.1-8B-Instruct/lora/train_2025-01-13-16-50-39/checkpoint-45/special_tokens_map.json
[INFO|trainer.py:2584] 2025-01-13 18:27:25,454 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                  100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 45/45 [1:36:21<00:00, 139.01s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 45/45 [1:36:21<00:00, 128.47s/it]
[INFO|trainer.py:3801] 2025-01-13 18:27:25,458 >> Saving model checkpoint to saves/Llama-3.1-8B-Instruct/lora/train_2025-01-13-16-50-39
[INFO|configuration_utils.py:679] 2025-01-13 18:27:26,341 >> loading configuration file config.json from cache at /home/ivieira/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659/config.json
[INFO|configuration_utils.py:746] 2025-01-13 18:27:26,342 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.1",
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|tokenization_utils_base.py:2646] 2025-01-13 18:27:27,041 >> tokenizer config file saved in saves/Llama-3.1-8B-Instruct/lora/train_2025-01-13-16-50-39/tokenizer_config.json
[INFO|tokenization_utils_base.py:2655] 2025-01-13 18:27:27,042 >> Special tokens file saved in saves/Llama-3.1-8B-Instruct/lora/train_2025-01-13-16-50-39/special_tokens_map.json
[INFO|trainer.py:4117] 2025-01-13 18:27:27,329 >> 
***** Running Evaluation *****
[INFO|trainer.py:4119] 2025-01-13 18:27:27,329 >>   Num examples = 2586
[INFO|trainer.py:4122] 2025-01-13 18:27:27,329 >>   Batch size = 8
/home/ivieira/miniconda3/envs/lf-env/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/home/ivieira/miniconda3/envs/lf-env/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
  0%|          | 0/324 [00:00<?, ?it/s]  1%|          | 2/324 [00:00<00:44,  7.24it/s]  1%|          | 3/324 [00:00<01:05,  4.90it/s]  1%|          | 4/324 [00:00<01:13,  4.35it/s]  2%|‚ñè         | 5/324 [00:01<01:18,  4.04it/s]  2%|‚ñè         | 6/324 [00:01<01:21,  3.92it/s]  2%|‚ñè         | 7/324 [00:01<01:21,  3.88it/s]  2%|‚ñè         | 8/324 [00:01<01:25,  3.72it/s]  3%|‚ñé         | 9/324 [00:02<01:24,  3.74it/s]  3%|‚ñé         | 10/324 [00:02<01:33,  3.36it/s]  3%|‚ñé         | 11/324 [00:02<01:29,  3.50it/s]  4%|‚ñé         | 12/324 [00:03<01:27,  3.57it/s]  4%|‚ñç         | 13/324 [00:03<01:27,  3.55it/s]  4%|‚ñç         | 14/324 [00:03<01:27,  3.56it/s]  5%|‚ñç         | 15/324 [00:04<01:34,  3.28it/s]  5%|‚ñç         | 16/324 [00:04<01:34,  3.25it/s]  5%|‚ñå         | 17/324 [00:04<01:32,  3.32it/s]  6%|‚ñå         | 18/324 [00:04<01:30,  3.38it/s]  6%|‚ñå         | 19/324 [00:05<01:30,  3.37it/s]  6%|‚ñå         | 20/324 [00:05<01:37,  3.12it/s]  6%|‚ñã         | 21/324 [00:06<01:51,  2.72it/s]  7%|‚ñã         | 22/324 [00:06<01:41,  2.96it/s]  7%|‚ñã         | 23/324 [00:06<01:33,  3.21it/s]  7%|‚ñã         | 24/324 [00:06<01:29,  3.36it/s]  8%|‚ñä         | 25/324 [00:07<01:32,  3.22it/s]  8%|‚ñä         | 26/324 [00:07<01:28,  3.36it/s]  8%|‚ñä         | 27/324 [00:07<01:26,  3.43it/s]  9%|‚ñä         | 28/324 [00:08<01:26,  3.43it/s]  9%|‚ñâ         | 29/324 [00:08<01:24,  3.49it/s]  9%|‚ñâ         | 30/324 [00:08<01:23,  3.54it/s] 10%|‚ñâ         | 31/324 [00:08<01:21,  3.61it/s] 10%|‚ñâ         | 32/324 [00:09<01:20,  3.62it/s] 10%|‚ñà         | 33/324 [00:09<01:17,  3.75it/s] 10%|‚ñà         | 34/324 [00:09<01:22,  3.54it/s] 11%|‚ñà         | 35/324 [00:09<01:21,  3.57it/s] 11%|‚ñà         | 36/324 [00:10<01:28,  3.26it/s] 11%|‚ñà‚ñè        | 37/324 [00:10<01:27,  3.29it/s] 12%|‚ñà‚ñè        | 38/324 [00:10<01:23,  3.44it/s] 12%|‚ñà‚ñè        | 39/324 [00:11<01:20,  3.54it/s] 12%|‚ñà‚ñè        | 40/324 [00:11<01:28,  3.21it/s] 13%|‚ñà‚ñé        | 41/324 [00:11<01:23,  3.37it/s] 13%|‚ñà‚ñé        | 42/324 [00:12<01:22,  3.41it/s] 13%|‚ñà‚ñé        | 43/324 [00:12<01:19,  3.52it/s] 14%|‚ñà‚ñé        | 44/324 [00:12<01:17,  3.63it/s] 14%|‚ñà‚ñç        | 45/324 [00:12<01:19,  3.49it/s] 14%|‚ñà‚ñç        | 46/324 [00:13<01:31,  3.03it/s] 15%|‚ñà‚ñç        | 47/324 [00:13<01:25,  3.23it/s] 15%|‚ñà‚ñç        | 48/324 [00:13<01:23,  3.31it/s] 15%|‚ñà‚ñå        | 49/324 [00:14<01:20,  3.44it/s] 15%|‚ñà‚ñå        | 50/324 [00:14<01:20,  3.42it/s] 16%|‚ñà‚ñå        | 51/324 [00:14<01:20,  3.40it/s] 16%|‚ñà‚ñå        | 52/324 [00:15<01:21,  3.35it/s] 16%|‚ñà‚ñã        | 53/324 [00:15<01:18,  3.43it/s] 17%|‚ñà‚ñã        | 54/324 [00:16<01:56,  2.31it/s] 17%|‚ñà‚ñã        | 55/324 [00:16<01:44,  2.57it/s] 17%|‚ñà‚ñã        | 56/324 [00:16<01:36,  2.79it/s] 18%|‚ñà‚ñä        | 57/324 [00:16<01:28,  3.01it/s] 18%|‚ñà‚ñä        | 58/324 [00:17<01:22,  3.21it/s] 18%|‚ñà‚ñä        | 59/324 [00:17<01:21,  3.27it/s] 19%|‚ñà‚ñä        | 60/324 [00:17<01:22,  3.21it/s] 19%|‚ñà‚ñâ        | 61/324 [00:18<01:19,  3.29it/s] 19%|‚ñà‚ñâ        | 62/324 [00:18<01:15,  3.47it/s] 19%|‚ñà‚ñâ        | 63/324 [00:18<01:13,  3.53it/s] 20%|‚ñà‚ñâ        | 64/324 [00:18<01:10,  3.71it/s] 20%|‚ñà‚ñà        | 65/324 [00:19<01:10,  3.67it/s] 20%|‚ñà‚ñà        | 66/324 [00:19<01:08,  3.77it/s] 21%|‚ñà‚ñà        | 67/324 [00:19<01:08,  3.78it/s] 21%|‚ñà‚ñà        | 68/324 [00:20<01:19,  3.23it/s] 21%|‚ñà‚ñà‚ñè       | 69/324 [00:20<01:16,  3.33it/s] 22%|‚ñà‚ñà‚ñè       | 70/324 [00:20<01:12,  3.50it/s] 22%|‚ñà‚ñà‚ñè       | 71/324 [00:21<01:51,  2.27it/s] 22%|‚ñà‚ñà‚ñè       | 72/324 [00:21<01:38,  2.56it/s] 23%|‚ñà‚ñà‚ñé       | 73/324 [00:21<01:29,  2.79it/s] 23%|‚ñà‚ñà‚ñé       | 74/324 [00:22<01:24,  2.97it/s] 23%|‚ñà‚ñà‚ñé       | 75/324 [00:22<01:20,  3.09it/s] 23%|‚ñà‚ñà‚ñé       | 76/324 [00:22<01:16,  3.24it/s] 24%|‚ñà‚ñà‚ñç       | 77/324 [00:23<01:22,  2.98it/s] 24%|‚ñà‚ñà‚ñç       | 78/324 [00:23<01:16,  3.21it/s] 24%|‚ñà‚ñà‚ñç       | 79/324 [00:23<01:11,  3.45it/s] 25%|‚ñà‚ñà‚ñç       | 80/324 [00:23<01:08,  3.58it/s] 25%|‚ñà‚ñà‚ñå       | 81/324 [00:24<01:09,  3.48it/s] 25%|‚ñà‚ñà‚ñå       | 82/324 [00:24<01:14,  3.25it/s] 26%|‚ñà‚ñà‚ñå       | 83/324 [00:24<01:09,  3.45it/s] 26%|‚ñà‚ñà‚ñå       | 84/324 [00:25<01:06,  3.60it/s] 26%|‚ñà‚ñà‚ñå       | 85/324 [00:25<01:06,  3.57it/s] 27%|‚ñà‚ñà‚ñã       | 86/324 [00:25<01:05,  3.65it/s] 27%|‚ñà‚ñà‚ñã       | 87/324 [00:26<01:13,  3.23it/s] 27%|‚ñà‚ñà‚ñã       | 88/324 [00:26<01:10,  3.35it/s] 27%|‚ñà‚ñà‚ñã       | 89/324 [00:26<01:08,  3.42it/s] 28%|‚ñà‚ñà‚ñä       | 90/324 [00:26<01:07,  3.47it/s] 28%|‚ñà‚ñà‚ñä       | 91/324 [00:27<01:06,  3.51it/s] 28%|‚ñà‚ñà‚ñä       | 92/324 [00:27<01:03,  3.66it/s] 29%|‚ñà‚ñà‚ñä       | 93/324 [00:27<01:05,  3.51it/s] 29%|‚ñà‚ñà‚ñâ       | 94/324 [00:27<01:04,  3.59it/s] 29%|‚ñà‚ñà‚ñâ       | 95/324 [00:28<01:06,  3.46it/s] 30%|‚ñà‚ñà‚ñâ       | 96/324 [00:28<01:04,  3.52it/s] 30%|‚ñà‚ñà‚ñâ       | 97/324 [00:28<01:01,  3.67it/s] 30%|‚ñà‚ñà‚ñà       | 98/324 [00:29<01:01,  3.67it/s] 31%|‚ñà‚ñà‚ñà       | 99/324 [00:29<01:04,  3.51it/s] 31%|‚ñà‚ñà‚ñà       | 100/324 [00:29<01:04,  3.49it/s] 31%|‚ñà‚ñà‚ñà       | 101/324 [00:29<01:02,  3.54it/s] 31%|‚ñà‚ñà‚ñà‚ñè      | 102/324 [00:30<00:59,  3.72it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 103/324 [00:30<01:01,  3.58it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 104/324 [00:30<01:02,  3.51it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 105/324 [00:31<01:02,  3.49it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 106/324 [00:31<01:02,  3.46it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 107/324 [00:31<01:00,  3.59it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 108/324 [00:31<01:00,  3.59it/s] 34%|‚ñà‚ñà‚ñà‚ñé      | 109/324 [00:32<00:58,  3.70it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 110/324 [00:32<00:57,  3.75it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 111/324 [00:32<00:56,  3.76it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 112/324 [00:32<00:56,  3.72it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 113/324 [00:33<00:55,  3.80it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 114/324 [00:33<00:58,  3.60it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 115/324 [00:33<00:56,  3.67it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 116/324 [00:34<00:56,  3.65it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 117/324 [00:34<00:55,  3.72it/s] 36%|‚ñà‚ñà‚ñà‚ñã      | 118/324 [00:34<00:59,  3.44it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 119/324 [00:34<00:59,  3.44it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 120/324 [00:35<00:55,  3.65it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 121/324 [00:35<00:54,  3.74it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 122/324 [00:35<00:55,  3.66it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 123/324 [00:35<00:54,  3.69it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 124/324 [00:36<00:54,  3.65it/s] 39%|‚ñà‚ñà‚ñà‚ñä      | 125/324 [00:36<00:54,  3.63it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 126/324 [00:36<00:54,  3.66it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 127/324 [00:37<00:54,  3.64it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 128/324 [00:37<00:56,  3.46it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 129/324 [00:37<00:56,  3.45it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 130/324 [00:37<00:55,  3.50it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 131/324 [00:38<00:54,  3.55it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 132/324 [00:38<00:52,  3.62it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 133/324 [00:38<00:51,  3.68it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 134/324 [00:39<00:53,  3.53it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 135/324 [00:39<00:51,  3.64it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 136/324 [00:39<00:50,  3.70it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 137/324 [00:39<00:50,  3.74it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 138/324 [00:40<00:52,  3.52it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 139/324 [00:40<00:52,  3.51it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 140/324 [00:40<00:50,  3.62it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 141/324 [00:40<00:48,  3.77it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 142/324 [00:41<00:48,  3.73it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 143/324 [00:41<00:47,  3.80it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 144/324 [00:42<01:00,  2.95it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 145/324 [00:42<00:57,  3.11it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 146/324 [00:42<00:53,  3.31it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 147/324 [00:42<00:51,  3.46it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 148/324 [00:43<00:50,  3.48it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 149/324 [00:43<00:48,  3.62it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 150/324 [00:43<00:47,  3.67it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 151/324 [00:43<00:47,  3.65it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 152/324 [00:44<00:51,  3.36it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 153/324 [00:44<00:50,  3.40it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 154/324 [00:44<00:54,  3.14it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 155/324 [00:45<01:28,  1.91it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 156/324 [00:46<01:14,  2.25it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 157/324 [00:46<01:04,  2.58it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 158/324 [00:46<00:58,  2.84it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 159/324 [00:47<00:56,  2.92it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 160/324 [00:47<00:53,  3.06it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 161/324 [00:47<00:51,  3.17it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 162/324 [00:47<00:48,  3.35it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 163/324 [00:48<00:47,  3.42it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 164/324 [00:48<00:45,  3.48it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 165/324 [00:48<00:44,  3.56it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 166/324 [00:48<00:43,  3.67it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 167/324 [00:49<00:42,  3.66it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 168/324 [00:49<00:42,  3.69it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 169/324 [00:49<00:41,  3.72it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 170/324 [00:49<00:41,  3.75it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 171/324 [00:50<00:42,  3.58it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 172/324 [00:50<00:41,  3.68it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 173/324 [00:50<00:40,  3.70it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 174/324 [00:51<00:49,  3.01it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 175/324 [00:51<00:47,  3.11it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 176/324 [00:51<00:44,  3.35it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 177/324 [00:52<00:42,  3.47it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 178/324 [00:52<00:40,  3.60it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 179/324 [00:52<00:41,  3.47it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 180/324 [00:52<00:40,  3.57it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 181/324 [00:53<00:39,  3.64it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 182/324 [00:53<00:39,  3.60it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 183/324 [00:53<00:39,  3.60it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 184/324 [00:54<00:37,  3.71it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 185/324 [00:54<00:36,  3.84it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 186/324 [00:54<00:36,  3.81it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 187/324 [00:54<00:36,  3.75it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 188/324 [00:55<00:36,  3.69it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 189/324 [00:55<00:35,  3.76it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 190/324 [00:55<00:35,  3.75it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 191/324 [00:55<00:34,  3.83it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 192/324 [00:56<00:35,  3.68it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 193/324 [00:56<00:35,  3.70it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 194/324 [00:56<00:36,  3.60it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 195/324 [00:56<00:35,  3.64it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 196/324 [00:57<00:35,  3.65it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 197/324 [00:57<00:33,  3.80it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 198/324 [00:57<00:33,  3.82it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 199/324 [00:57<00:32,  3.89it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 200/324 [00:58<00:33,  3.68it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 201/324 [00:58<00:32,  3.76it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 202/324 [00:58<00:33,  3.70it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 203/324 [00:59<00:32,  3.71it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 204/324 [00:59<00:32,  3.73it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 205/324 [00:59<00:31,  3.73it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 206/324 [00:59<00:34,  3.46it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 207/324 [01:00<00:33,  3.51it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 208/324 [01:00<00:31,  3.64it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 209/324 [01:00<00:32,  3.57it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 210/324 [01:01<00:30,  3.68it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 211/324 [01:01<00:30,  3.71it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 212/324 [01:01<00:29,  3.81it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 213/324 [01:01<00:29,  3.79it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 214/324 [01:02<00:28,  3.80it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 215/324 [01:02<00:29,  3.75it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 216/324 [01:02<00:28,  3.79it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 217/324 [01:02<00:28,  3.72it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 218/324 [01:03<00:28,  3.75it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 219/324 [01:03<00:28,  3.71it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 220/324 [01:03<00:27,  3.75it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 221/324 [01:03<00:27,  3.79it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 222/324 [01:04<00:26,  3.79it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 223/324 [01:04<00:26,  3.84it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 224/324 [01:04<00:27,  3.70it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 225/324 [01:05<00:26,  3.72it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 226/324 [01:05<00:25,  3.82it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 227/324 [01:05<00:25,  3.82it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 228/324 [01:05<00:25,  3.81it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 229/324 [01:06<00:24,  3.92it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 230/324 [01:06<00:24,  3.80it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 231/324 [01:06<00:24,  3.84it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 232/324 [01:06<00:23,  3.85it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 233/324 [01:07<00:23,  3.84it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 234/324 [01:07<00:24,  3.72it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 235/324 [01:07<00:24,  3.61it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 236/324 [01:07<00:24,  3.65it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 237/324 [01:08<00:23,  3.68it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 238/324 [01:08<00:24,  3.52it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 239/324 [01:08<00:23,  3.56it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 240/324 [01:09<00:23,  3.59it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 241/324 [01:09<00:23,  3.48it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 242/324 [01:09<00:24,  3.40it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 243/324 [01:09<00:23,  3.40it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 244/324 [01:10<00:23,  3.45it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 245/324 [01:10<00:22,  3.59it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 246/324 [01:10<00:21,  3.57it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 247/324 [01:11<00:20,  3.68it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 248/324 [01:11<00:21,  3.62it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 249/324 [01:11<00:20,  3.62it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 250/324 [01:11<00:20,  3.56it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 251/324 [01:12<00:20,  3.65it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 252/324 [01:12<00:19,  3.69it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 253/324 [01:12<00:19,  3.73it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 254/324 [01:12<00:18,  3.80it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 255/324 [01:13<00:18,  3.79it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 256/324 [01:13<00:18,  3.67it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 257/324 [01:13<00:17,  3.75it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 258/324 [01:14<00:18,  3.65it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 259/324 [01:14<00:17,  3.72it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 260/324 [01:14<00:16,  3.79it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 261/324 [01:14<00:17,  3.62it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 262/324 [01:15<00:18,  3.39it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 263/324 [01:15<00:17,  3.42it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 264/324 [01:15<00:17,  3.49it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 265/324 [01:16<00:16,  3.55it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 266/324 [01:16<00:16,  3.56it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 267/324 [01:16<00:15,  3.58it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 268/324 [01:16<00:15,  3.60it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 269/324 [01:17<00:15,  3.64it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 270/324 [01:17<00:14,  3.73it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 271/324 [01:17<00:14,  3.71it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 272/324 [01:17<00:14,  3.57it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 273/324 [01:18<00:14,  3.57it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 274/324 [01:18<00:14,  3.37it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 275/324 [01:18<00:14,  3.45it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 276/324 [01:19<00:13,  3.54it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 277/324 [01:19<00:13,  3.60it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 278/324 [01:19<00:12,  3.65it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 279/324 [01:19<00:12,  3.71it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 280/324 [01:20<00:11,  3.76it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 281/324 [01:20<00:11,  3.76it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 282/324 [01:20<00:11,  3.68it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 283/324 [01:20<00:11,  3.63it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 284/324 [01:21<00:10,  3.67it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 285/324 [01:21<00:10,  3.71it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 286/324 [01:21<00:10,  3.61it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 287/324 [01:22<00:10,  3.60it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 288/324 [01:22<00:10,  3.48it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 289/324 [01:22<00:09,  3.67it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 290/324 [01:22<00:09,  3.65it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 291/324 [01:23<00:08,  3.76it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 292/324 [01:23<00:08,  3.76it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 293/324 [01:23<00:08,  3.78it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 294/324 [01:24<00:09,  3.18it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 295/324 [01:24<00:08,  3.26it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 296/324 [01:24<00:08,  3.36it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 297/324 [01:24<00:07,  3.47it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 298/324 [01:25<00:07,  3.58it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 299/324 [01:25<00:06,  3.59it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 300/324 [01:25<00:06,  3.56it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 301/324 [01:26<00:06,  3.56it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 302/324 [01:26<00:06,  3.65it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 303/324 [01:26<00:06,  3.41it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 304/324 [01:26<00:05,  3.43it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 305/324 [01:27<00:05,  3.59it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 306/324 [01:27<00:05,  3.59it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 307/324 [01:27<00:04,  3.69it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 308/324 [01:27<00:04,  3.68it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 309/324 [01:28<00:04,  3.62it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 310/324 [01:28<00:03,  3.52it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 311/324 [01:28<00:03,  3.70it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 312/324 [01:29<00:03,  3.74it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 313/324 [01:29<00:02,  3.75it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 314/324 [01:29<00:03,  3.30it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 315/324 [01:29<00:02,  3.43it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 316/324 [01:30<00:02,  3.54it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 317/324 [01:30<00:01,  3.56it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 318/324 [01:30<00:01,  3.65it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 319/324 [01:31<00:01,  3.62it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 320/324 [01:31<00:01,  3.61it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 321/324 [01:31<00:00,  3.57it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 322/324 [01:31<00:00,  3.47it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 323/324 [01:32<00:00,  3.56it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 324/324 [01:32<00:00,  3.84it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 324/324 [01:32<00:00,  3.51it/s]
[INFO|modelcard.py:449] 2025-01-13 18:29:00,097 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}
